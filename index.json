[{"uri":"https://tiendon.github.io/aws-worklog/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Modernizing your application isn\u0026rsquo;t difficult: Migrating to Amazon EKS with your existing NLB configuration Authors:\nHenrique Santana, Container Specialist, AWS\nLuis Felipe, Core Solution Architect, AWS\nDate: March 25, 2025\nIntroduction Many organizations have built their infrastructure using Amazon Elastic Compute Cloud (Amazon EC2) and Network Load Balancer (NLB), often with security policies built around the NLB\u0026rsquo;s static IP address. When these organizations adopt containerization and switch to Amazon Elastic Kubernetes Service (Amazon EKS) for their modern applications, they face a significant challenge in maintaining their existing endpoint configuration. This can make modernization complicated and risky, as changes to load balancing configurations can disrupt client connectivity or require major DNS changes.\nThe good news is that this transition doesn\u0026rsquo;t necessarily have to be an all-or-nothing effort. In this post, we explore a hybrid deployment pattern that provides a smooth, low-risk migration path from Amazon EC2 to Amazon EKS.\nHybrid Deployment Similar to the blue/green deployment pattern, our hybrid deployment approach supports running the application concurrently on Amazon EKS and Amazon EC2 during the migration. The key to this strategy is the ability to route traffic through your existing load balancer to both deployments using TargetGroupBinding, resulting in a controlled migration process.\nArchitecture Figure 1: Traffic flow diagram showing migration from Amazon EC2 (green) to Amazon EKS (blue)\nAdvantages of using a hybrid deployment approach {: style=\u0026ldquo;font-size: 18px;\u0026rdquo;} The hybrid deployment approach offers the following advantages:\n• Controlled migration: Gradually route traffic to the EKS workload while maintaining service through the existing EC2 infrastructure With your own approach, significantly reduce migration risk.\n• Simple Rollback: Quickly redirect traffic back to EC2 instances if issues arise, providing reliable rollback during migration.\n• A/B Testing: Compare performance between EC2 and EKS deployments under real-world conditions, providing data-driven decisions on the most efficient configuration and resource allocation.\n• Flexibility: Leverage the strengths of both deployment environments during the migration, optimizing workload placement based on specific requirements.\n• Minimized Service Interruption: Reduce downtime risk by running both environments concurrently during migration.\n• Risk Reduction: Validate containerized deployments with real traffic while maintaining fallback options, providing business continuity.\nPrerequisites for Migration This post is written assuming you have a basic understanding of Docker containerization and Kubernetes concepts (pods, events, namespaces, and deployments, etc.).\nBefore beginning the migration process, ensure you have the following components available:\n• Existing Infrastructure: In our example, a Retail Store sample application is running on Amazon EC2.\n• Container Requirements: A tested and containerized application, with the container image pushed to the container registry, such as Amazon Elastic Container Registry (Amazon ECR).\n• Amazon EKS Environment: EKS cluster with supported Kubernetes version across multiple Availability Zones (AZs).\n• Tools: ◦ kubectl has been installed and configured to interact with the Amazon EKS cluster.\n◦ AWS Command Line Interface (AWS CLI) has been installed and configured with the appropriate AWS Identity and Access Management (IAM). ◦ AWS Load Balancer Controller has been installed on the Amazon EKS cluster.\nMigrate your application using a hybrid deployment with NLB {: style=\u0026ldquo;font-size: 18px;\u0026rdquo;} Detailed migration steps {: style=\u0026ldquo;font-size: 16px;\u0026rdquo;} The first step is to create a new target group:\nTARGET_GROUP_ARN=$(aws elbv2 create-target-group \\ --name eks-green \\ --protocol TCP \\ --port 80 \\ --target-type ip \\ --vpc-id \u0026lt;VPCID\u0026gt; \\ --query \u0026#39;TargetGroups[0].TargetGroupArn\u0026#39; \\ --output text) The ARN (Amazon Resource Name) of the target group is stored in the TARGET_GROUP_ARN variable. Next, verify that both target groups (Amazon EC2 and Amazon EKS) are configured with Terminate connections on deregistration set to true.\naws elbv2 describe-target-group-attributes \\ --target-group-arn $TARGET_GROUP_ARN \\ --query \u0026#39;Attributes[*]\u0026#39; --output text | \\ grep deregistration_delay.connection_termination.enabled Observe the following output:\nderegistration_delay.connection_termination.enabled true\nIf the command output returns false, it means this feature is not enabled. To enable it, use the following command:\naws elbv2 modify-target-group-attributes \\ --target-group-arn $TARGET_GROUP_ARN \\ --attributes \u0026#39;Key=deregistration_delay.connection_termination.enabled,Value=true\u0026#39; \\ --query \u0026#39;Attributes[*]\u0026#39; --output text | \\ grep deregistration_delay.connection_termination.enabled At this stage, traffic is still directed to the application running on Amazon EC2. Now you can deploy the containerized application on the Amazon EKS cluster:\nkubectl create deployment myapp \\ --image public.ecr.aws/aws-containers/retail-store-sample-ui:0.8.1 \\ --replicas 2 --port=8080 We can expose the application by creating a service. This is the YAML file for the service:\ncat \u0026lt;\u0026lt; EOF \u0026gt; service.yaml apiVersion: v1 kind: Service metadata: labels: app: myapp name: ui-service namespace: default spec: ports: - port: 80 targetPort: 8080 selector: app: myapp type: ClusterIP EOF We can apply it:\nkubectl apply -f service.yaml With the application running and exposed as a Kubernetes service, create a new listener for the existing NLB:\naws elbv2 create-listener \\ --load-balancer-arn \u0026lt;NLB-ARN\u0026gt; \\ --protocol TCP --port 81 \\ --default-actions Type=forward,TargetGroupArn=$TARGET_GROUP_ARN Currently, you have a listener on port 80. This listener continues to forward traffic to the target group associated with EC2 instances. The newly created listener forwards traffic to the target group associated with Amazon EKS. It doesn\u0026rsquo;t have any target until you use TargetGroupBinding to bind the service to the new target group. We can create it:\ncat \u0026lt;\u0026lt; EOF \u0026gt; tg-binding.yaml apiVersion: elbv2.k8s.aws/v1beta1 kind: TargetGroupBinding metadata: name: ui-tgbinding spec: serviceRef: name: ui-service # Service name port: 80 # Service port targetGroupARN: $TARGET_GROUP_ARN EOF Apply manifest:\nkubectl apply -f tg-binding.yaml You can verify that the targets are properly registered to the Amazon EKS target group. Give the targets enough time to pass the health check, then verify that the new targets can successfully serve traffic. If the health check fails, verify that the Amazon EKS node\u0026rsquo;s Security Group allows the necessary traffic. To confirm proper functionality, try accessing the application through the new listener to ensure it\u0026rsquo;s being served as intended before proceeding with the addition.\nBefore starting the migration, you need to create another listener pointing to the existing EC2 target group:\naws elbv2 create-listener \\ --load-balancer-arn \u0026lt;NLB-ARN\u0026gt; \\ --protocol TCP --port 82 \\ --default-actions Type=forward,TargetGroupArn=\u0026lt;EC2-TARGET-GROUP-ARN\u0026gt; After this step, NLB has three listeners (ports 80, 81, and 82), where you\u0026rsquo;ve created two more listeners to perform the smooth traffic migration. The port numbers used in this post are examples, and you can choose port numbers to reflect your application\u0026rsquo;s needs. We recommend checking if traffic to existing target groups is being served on the new listener. This ensures that the NLB configuration is ready to proceed with the next steps.\nMoving traffic from Amazon EC2 to Amazon EKS target group {: style=\u0026ldquo;font-size: 16px;\u0026rdquo;} Both target groups are healthy, and all listeners are ready to handle traffic, so it\u0026rsquo;s time to move the traffic. First, configure the existing listener on port 80 to send traffic to the eks-green target group:\naws elbv2 modify-listener \\ --listener-arn \u0026lt;NLB-Listen-80\u0026gt; \\ --default-actions Type=forward,TargetGroupArn=$TARGET_GROUP_ARN Changing this configuration ensures that all new flows are redirected to the new target group. Because the targets passed their health check when linked to the listener on port 81, this helps This will speed up the process.\nThis listener change may take a few minutes to propagate. Therefore, we recommend monitoring traffic going to targets in the new target group before proceeding with the next steps. During this time, it\u0026rsquo;s important to monitor your application\u0026rsquo;s behavior and performance metrics. You can monitor the target\u0026rsquo;s health status using:\naws elbv2 describe-target-health \\ --target-group-arn $TARGET_GROUP_ARN While existing connections will not be affected, new connections will stop routing to the old target group after the traffic migration is complete. After confirming that the new targets are successfully handling traffic and that the previously existing targets are no longer serving traffic, retrieve a list of targets from the previously existing EC2 target group to prepare for deregistration:\naws elbv2 describe-target-health \\ --target-group-arn \u0026lt;EC2-TARGET-GROUP-ARN\u0026gt; \\ --query \u0026#39;TargetHealthDescriptions[*].Target.Id\u0026#39; \\ --output text This command outputs a list of instance IDs. For each instance ID, run the deregister-targets command:\naws elbv2 deregister-targets \\ --target-group-arn \u0026lt;EC2-TARGET-GROUP-ARN\u0026gt; \\ --targets Id=\u0026lt;InstanceID\u0026gt; The deregistration step is crucial because the EC2 target group remains associated with NLB via the port 82 listener. When executed, the deregister-targets call performs a Connection termination on deregistration, which terminates existing connections to the old targets. When clients reconnect, they are routed to the targets of the new Amazon EKS target group.\nRollback to Original Target Group Procedure (If Needed) {: style=\u0026ldquo;font-size: 16px;\u0026rdquo;} If you need to roll back to the original EC2 target group, follow these steps:\nRe-register all original targets to the EC2 target group: aws elbv2 register-targets \\ --target-group-arn \u0026lt;EC2-TARGET-GROUP-ARN\u0026gt; \\ --targets Id=\u0026lt;InstanceID1\u0026gt; Id=\u0026lt;InstanceID2\u0026gt; Wait for the targets to pass the health check. Monitor their health status: aws elbv2 describe-target-health \\ --target-group-arn \u0026lt;EC2-TARGET-GROUP-ARN\u0026gt; Reconfigure the listener on port 80 on NLB to send traffic to the target group EC2: aws elbv2 modify-listener \\ --listener-arn \u0026lt;NLB-Listen-80\u0026gt; \\ --default-actions Type=forward,TargetGroupArn=\u0026lt;EC2-TARGET-GROUP-ARN\u0026gt; Cleanup after migration {: style=\u0026ldquo;font-size: 16px;\u0026rdquo;} If the migration was successful (no rollback needed), then you can proceed with cleanup operations. This includes deleting the two additional listeners you created on ports 81 and 82, as they were only necessary for the migration process. Finally, you can safely delete the EC2 target group, as it no longer receives any traffic.\naws elbv2 delete-listener \\ --listener-arn \u0026lt;Listen-Port81-ARN\u0026gt; aws elbv2 delete-listener \\ --listener-arn \u0026lt;Listen-Port82-ARN\u0026gt; aws elbv2 delete-target-group \\ --target-group-arn \u0026lt;EC2-TG-ARN\u0026gt; Conclusion The hybrid deployment pattern using NLB and TargetGroupBinding provides a practical, low-risk approach to migrating applications to Amazon EKS from various sources, including Amazon EC2, on-premises infrastructure, or other container orchestration solutions. Maintaining the existing NLB configuration while gradually transitioning traffic to containerized workloads allows this approach to support a smooth transition and provide integrated rollback capabilities. While we focused on migrating from Amazon EC2 to Amazon EKS, the flexibility of this pattern extends to many different scenarios, including transitions from on-premises infrastructure or other container orchestration solutions.\nRecent improvements to the AWS Load Balancer Controller, particularly the introduction of the target group MultiCluster, further expand these capabilities. Organizations can now manage workloads across multiple Kubernetes clusters and integrate with non-cluster resources, facilitating more complex migration strategies and distributed application architectures. This hybrid approach serves as a reliable blueprint for modernization, providing the necessary tools to maintain business continuity and mitigate risk while offering the flexibility to adapt to evolving infrastructure requirements.\nTo continue your migration journey, we recommend reading our companion post: Migrating from Self-Managed Kubernetes to Amazon EKS: Here Are Some Key Considerations. This post provides further insights and best practices that complement the hybrid deployment strategy discussed here, especially if you are migrating from a self-managed Kubernetes environment to Amazon EKS.\nTAGS: EKS, elb, nlb\n"},{"uri":"https://tiendon.github.io/aws-worklog/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Enhance LLM observability with Amazon Bedrock and Dynatrace Author:\nKristof Muhi, Principal Product Manager – Dynatrace Varun Jasti, Solutions Architect – AWS Shashiraj Jeripotula, Principal Partner Solutions Architect – AWS Date: March 27, 2025\nIntroduction Organizations leveraging Amazon Bedrock for their AI generation applications need to ensure reliable, secure, and responsible AI operations at scale. As these applications become an integral part of business processes, the deployment of comprehensive Large Language Model (LLM) observability becomes essential. Monitoring model performance, detecting hallucinations, prompt injection attacks, malicious language, and PII leaks, while also tracking latency, drift, data flow, and maintaining cost control, are some of the critical use cases. By implementing robust observability practices, teams can gain deep insights into their LLM application behavior, optimize resource usage, ensure consistent response quality, and maintain compliance with governance requirements.\nDynatrace is an all-in-one observation platform that automatically collects production insights, tracing, logging, metrics, and real-time application data at scale. With its powerful AI engine (Davis AI), Dynatrace alerts the team to production-level issues before they disrupt users, helps predict resource usage and costs, performance issues, and provides safeguards for data protection and compliance maintenance.\nIn this post, we explain how Dynatrace delivers end-to-end monitoring and visibility into AI-generated applications using Amazon Bedrock models that enable comprehensive LLM observation capabilities.\nLLM Observational Use Cases Dynatrace assists with the following large-scale LLM observation and AI generation use cases.\nComplexity of Multi-Model Tracing Multi-model tracing presents hidden complexities as interactions between models with different architectures, output formats, and latency profiles must be coherently correlated across the entire chain. When diverse models operate in a chain, errors can silently propagate through these heterogeneous systems, making root cause analysis particularly challenging without standardized telemetry that can efficiently connect the points between different inputs and outputs.\nDynatrace enables end-to-end tracing across different models, connecting the frontend and backend components of the application stack.\nThis multi-model tracing provides complete visibility and traceability of events, allowing you to understand what happened when an issue occurred or when an invalid response was sent to the client throughout the entire model chain.\nPredictive Operations of AI Workloads – Cost and Performance Predictive operations leverage advanced analytics and machine learning to predict and optimize AI workload behavior before issues impact business operations, powered by Davis AI. This proactive approach transforms traditional monitoring into forward-looking operational intelligence for AI systems.\n• Cost Forecasting and Optimization: Forecast token usage, API calls, and associated costs within Amazon Bedrock to enable better budget planning and resource allocation for the future. Efficient budget planning with accurate cost forecasting helps reduce operational costs through better resource planning and allocation.\n• Performance Downturn Prediction: Identify early warning signs of potential model performance issues through pattern recognition.\n• Anomaly and Problem Detection: Use Dynatrace\u0026rsquo;s Davis AI Prediction to detect anomalous patterns in model behavior that may indicate emerging problems or peak usage in the architecture. This will minimize service and application downtime by addressing potential issues before they become critical.\nBarrier Analysis Barrier analysis focuses on monitoring and enforcing safety boundaries around AI systems to ensure they operate within defined ethical, security, and performance parameters. This critical capability helps organizations maintain control over their AI applications while protecting against potential risks and abuse.\n• Key components such as real-time detection of prompt injection attack attempts and security vulnerabilities protect your business and customer data. This allows organizations to monitor unauthorized PII exposures and leaks of sensitive data.\n• Barriers allow you to monitor malicious and inappropriate language, harmful content, and other harmful material. • Bias feedback with early threat detection. Allows for improved model reliability through consistent boundary enforcement and easy compliance.\n• Barrier analysis protects AI applications by safeguarding brand reputation through preventing inappropriate feedback and queries.\nData Governance, Compliance, and Auditing In the context of applications built with LLM on Amazon Bedrock, governance, compliance, and auditing capabilities through observability ensure organizations maintain control, transparency, and accountability for their AI-generated applications while meeting regulatory requirements and industry standards.\nDynatrace helps track all inputs and outputs for a full audit trail. It allows you to query all data in real time and store it for future reference. It easily establishes and maintains a complete data stream from prompt to response across the entire pipeline and gathers all evidence for responsible AI practices and regulatory reporting such as FIPS, FedRAMP and EU AI Act.\nDynatrace\u0026rsquo;s model fingerprinting capability generates unique identifiers for LLM instances based on architecture, training data, and parameters, enabling accurate instance tracking for regulatory and audit compliance. Accurate tracking of model instances through model fingerprinting ensures compliance with regulatory and audit requirements.\nDynatrace + LLM Observation Layers LLM observation requires an expansive approach spanning multiple layers, from user-oriented applications to underlying infrastructure. Each layer plays a crucial role in understanding LLM performance, identifying bottlenecks, ensuring reliable operation, and detecting potential security risks. Dynatrace provides a unified end-to-end observation platform that can help organizations gain deep insights into each of these layers, enabling them to effectively monitor, optimize, troubleshoot, and secure their LLM-powered applications.\nFigure 1: Bedrock Observation Pipeline of a Travel Application Running on Kubernetes Using Dynatrace\nIn our example, the application runs in a Kubernetes cluster. OpenLLMetry of Traceloop enhances LLM observation capabilities for Amazon Bedrock models by collecting specific KPIs about key AI. It enriches OpenTelemetry data and integrates seamlessly with Dynatrace. This provides a comprehensive view of LLM application performance in a production environment. Ultimately, it empowers businesses to optimize and scale their AI deployments efficiently.\nFigure 2: High-level overview of the different layers of AI-generated measurement devices for observational capabilities\nFrom the diagram above (Figure 2), Dynatrace provides end-to-end visibility of AI applications through the entire technology stack.\n• Application Layer: Dynatrace monitors user-oriented applications interacting with LLM, tracking performance metrics, user experience, and usage patterns. Continuous data collection across the entire architecture (frontend, backend, AI generation stack) reveals real-time application behavior, while logging collects user interactions and application-specific errors for debugging. Visualization tools provide customizable dashboards to monitor key application metrics and identify trends or anomalies related to LLM integration.\n• Organization Layer – Monitoring the Performance of the Orchestration Framework: These frameworks (e.g., LangChain, LlamaIndex) manage the workflow prompt and integration pipeline. Dynatrace observes these workflows, providing metrics on prompt engineering efficiency, chain performance, and caching. Its anomaly detection capabilities alert the team to potential issues and bottlenecks, ensuring the smooth operation of AI-driven processes.\n• Semantic Layer and Vector Databases: Analyzes the meaning and content of LLM inputs and outputs. This involves understanding relationships between concepts, tracking sentiment, and identifying potential biases, inaccuracies, or anomalies along with performance bottlenecks in Retrieval Augmented Generation (RAG) architectures using vector databases (e.g., Pinecone, Milvus, Weaviate, Qdrant, Chroma). Dynatrace\u0026rsquo;s extensibility allows for integration. Compatible with semantic analytics tools. By gathering data from a vector database, Dynatrace can provide a unified view of LLM output, including sentiment analysis, topic modeling, and bias detection. This data can then be visualized and analyzed using Dynatrace\u0026rsquo;s Metrics \u0026amp; Performance Analytics and Visualization capabilities.\n• Model Layer – Observing Model Providers and Platform Providers: Dynatrace monitors token usage, stability, latency, throughput, resource consumption, and model drift within Amazon Bedrock. Metrics \u0026amp; Performance Analytics allows for deep dives into model performance, tracking metrics such as latency, throughput, and resource consumption. Model fingerprinting supports detailed versioning, while anomaly detection flags significant changes in performance or output quality—helping the team understand and optimize model behavior.\n• Infrastructure Layer: Dynatrace\u0026rsquo;s full-stack observation capabilities extend to computing resources (e.g., Amazon EC2, NVIDIA GPUs) and networks. It automatically collects CPU/GPU utilization, memory usage, and other critical statistics. Real-time anomaly detection with Davis AI helps teams quickly address hardware bottlenecks that could impact LLM performance.\n\u0026ldquo;From initial model evaluation to production deployment, comprehensive monitoring is critical for generative AI systems. The integration between Dynatrace and Amazon Bedrock allows organizations to easily track key performance metrics and trace data, ensuring their AI applications remain optimized and operate reliably.\u0026rdquo; – Denis Batalov, Tech Leader, ML \u0026amp; AI, AWS.\n\u0026ldquo;Generative AI is rapidly becoming the standard for customer experience, driving companies to deliver AI-native interactions at high speed. Simultaneously, we are witnessing an accelerated development of AI systems, leading to exponentially increasing capabilities. However, deploying these highly complex AI application stacks in production presents significant challenges. AI observational capabilities play a crucial role in ensuring reliable performance, enhancing customer satisfaction, and driving measurable ROI for the business.\u0026rdquo; – Alois Reitbauer, VP, Chief Technology Strategist, Dynatrace\nSummary In this blog post, we discussed how Dynatrace can deliver enhanced visibility into generative AI applications leveraging Amazon Bedrock.\nLeveraging Dynatrace\u0026rsquo;s capabilities, you can:\n• Maintain operational efficiency: Monitor latency, resource usage, and costs to optimize performance and control expenses.\n• Accelerate the path to production: Deploy reliable and secure AI applications faster with confidence.\n• Make data-driven decisions: Leverage comprehensive data to inform model selection, fine-tuning, and risk mitigation strategies.\n• Improve application reliability: Proactively identify and address performance bottlenecks and other issues before they impact users.\n• Enhance security: Detect and mitigate security risks in real time, protecting your data and reputation.\n• Enhancing Governance and Compliance: Maintain a clear data flow and ensure responsible AI practices that meet regulatory requirements and ethical standards.\nFor guidance on setting up Dynatrace\u0026rsquo;s end-to-end instrumentation solution with Amazon Bedrock, please refer to the following Dynatrace blog post.\nDynatrace – AWS Partner Spotlight Dynatrace is an AWS Advanced Technology Partner and AWS Competency Partner providing intelligent software to simplify cloud complexity and accelerate digital transformation. With enhanced observational capabilities, AI, and full automation, our all-in-one platform provides answers—not just data—about application performance, underlying infrastructure, and the experience of all users.\nContact Dynatrace | Partner Overview | AWS Marketplace\nTAGS: Amazon Bedrock, Artificial Intelligence, Machine Learning, Observability\n"},{"uri":"https://tiendon.github.io/aws-worklog/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Empower Your Teams with Modern Architecture Governance Author:\nRostislav Markov, Principal Architect – AWS Professional Services\nDate: April 21, 2025\nAgile product teams thrive on autonomy and rapid iteration, especially in the cloud where they can quickly deploy and test systems. However, traditional architecture governance often hinders them, as many businesses still impose centralized, one-time architecture approval processes at the beginning of the process. Historically, these approval processes verified design compliance with enterprise standards in a slower-paced, on-premises world. In a cloud environment, such approval processes quickly become obsolete—along with the associated architectural documents—and prevent teams from considering new insights.\nModern cloud architecture demands a new governance approach. In this post, we show how collaborative architecture oversight can transform team performance through automation, self-service platforms, and distributed decision-making. We explore how key stakeholders (developers, architects, security experts, and shared services teams) can participate in architectural decisions through asynchronous approval processes, while ensuring non-negotiable controls such as encryption at break and during transport are consistently enforced through automation and policies as code. This approach empowers teams to experiment and adapt quickly while maintaining strong enterprise standards.\nThe Promise of Traditional Architecture Approval Processes Traditional architecture governance centers around formal reviews where teams submit detailed design documents to a central architecture board. These artifacts typically include comprehensive schematics, technology selections, security plans, and integration specifications. Architects and a range of stakeholders such as security experts, compliance officers, quality assurance, and operations teams review these documents in scheduled meetings before submitting them to the approval process. These approvals represent validation at a specific point in time against enterprise standards, assuming minimal deviations during implementation.\nWhy Traditional Approval Processes Are Faulty Approval processes can create challenges in modern cloud architecture:\n• Substituting for continuous compliance when lacking automated verification, creating false assurances through one-time reviews\n• Creating a restrictive \u0026ldquo;checkbox\u0026rdquo; mentality where meeting minimum documentation requirements becomes the goal instead of exploring best practices\n• Stripping decision-making power from deployment teams who typically possess the most contextual knowledge\n• Slowing the deployment feedback loop and reducing organizational agility\nConsider an agile team responsible for a strategic cloud application that has passed the minimum viable product and is now scaling to support growing business needs. The system architecture must evolve to handle the increasing data volume, performance requirements, and unpredictable integrations. Yet, business stakeholders insist on strict adherence to the initially approved architecture documents. Although governance is essential for production systems, this inflexible approach to early architectural approval processes prevents the team from implementing architectural improvements as they progress. What appears to be meaningful control for stakeholders becomes a suffocating constraint for builders, ultimately harming the system stability that governance is intended to protect.\nSupporting Modern Cloud Architecture A modern architectural function operates around development capabilities across three core areas: pre-approved blueprints, distributed governance, and automated information—with traditional approval processes reserved as an exception for unique use cases.\nFigure 1: Areas supporting modern cloud architecture\nPre-approved Blueprints Pre-approved blueprints as reference architectures and code templates allow your teams to move faster while maintaining enterprise standards. This approach supports the model Use case-focused assessments allow architects to concentrate on evaluating specific workflows or threats relevant to a unique use case—rather than having to understand the entire system or threat model from scratch. In this way, the architectural function shifts to exception-based management and refocuses assessments on deviations from the standard. Blueprints should have gravity, guiding teams toward standardized patterns while preventing fragmentation across too many tools, databases, middleware options, or SDKs. Consider the following:\n• Pattern-based reference architectures – These establish clear principles for security and resilience without micromanagement. These standards link teams while enabling innovation within a trusted framework. Cloud-Driven Enterprise Transformation at BMW Group illustrates how the transition from approval processes to empowerment through template-based architecture can be successful.\n• Self-Serve Platforms – These provide standardized resources that empower independent build teams. A self-service platform with pre-approved templates for deployment toolchains and infrastructure code allows for confident and rapid development. Most companies host these on internal developer platforms such as Backstage or AWS Service Catalog. This also allows for controlled changes to the blueprint and monitoring their implementation.\n• Machinery Lifecycle – Blueprints require their own approval process. While this creates significant efficiency by reducing individual system reviews, it presents the challenge of managing existing implementations as patterns are updated. This includes versioning and migration strategies when introducing new blueprints.\nDistributed Governance Distributed governance views architectural decision-making as a continuous, collaborative process with clear accountability, delegating decision-making and empowering your builders within established blueprints. Consider the following:\n• Architectural Decision Records (ADRs) – These replace one-time, formal approval processes by documenting decisions for each build iteration. This approach promotes transparency and maintains team agility without compromising accountability for decisions and approvals with key stakeholders. It also allows teams to defer decisions until they are most relevant. For practical implementation guidance, see Using Architectural Decision Records to Streamline Decision Making for a Software Development Project. To learn about writing concise and duplicate ADRs, refer to the GitHub repository ADR template and When Should I Write an Architectural Decision Record.\n• Community-Driven Consultation – Architectural departments can foster self-organization by creating architectural practice communities to share peer knowledge. These communities allow collaboration on best practices, challenges, and standards, fostering a distributed decision-making culture, without eliminating lines of responsibility and accountability for final decisions. This approach works because deep architectural expertise is often found in builders with practical experience with specific use cases and technologies. The role of the architectural function shifts to providing the necessary infrastructure and identifying thought leaders within the organization.\nAutomated Information Automated information enables compliance with enterprise standards through real-time monitoring and adaptation:\n• Continuous Monitoring – Continuous workload discovery detects architecture based on log data such as AWS Config, AWS CloudTrail, VPC Flow Logs and Amazon GuardDuty. Gathering information from the application environment allows the architecture function to automatically generate architecture diagrams, embed compliance policies as code such as AWS Config rules, and provide real-time security checks such as the Workload Discovery on AWS solution, which can automatically generate • Architectural flowcharts and cost reports for AWS accounts. Refer to the AWS Partner Solutions Finder to explore partner solutions for application discovery and monitoring.\n• AI-driven governance – AI tools can analyze decisions, identify architectural and code inefficiencies, detect anomalies, and recommend optimized configurations. This supports informed decision-making, especially when supplemented with thorough human verification and oversight. Amazon Bedrock Agents can find similar existing ADRs, analyze architectural schemas, and generate infrastructure code. For example, the Japan Digital Agency uses AI assistants to streamline migration assessments for hundreds of systems.\n(https://aws.amazon.com/blogs/publicsector/japans-digital-agency-accelerates-government-cloud-migration-with-aws-generative-ai-powered-architecture-reviews/) ## Comparison\nThe modern perspective improves the value-added model and overall support of your architectural function. The following table compares the traditional and modern perspectives.\n| Aspect | Traditional Perspective | Modern Perspective |\n| \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;\n| Purpose | Centralized approval to enforce control and reduce risk | Empowering teams with pre-approved standards to prevent spillovers and manage their distribution, such as through Backstage |\n| Architectural Approach | Fixed, one-time design | Development, treated as a reusable, parameterized code product refined through feedback |\n| Team Empowerment | Restricted, decisions approved by a centralized authority | High, teams make decisions within clear standards |\n| Team Speed ​​and Agility | Slower, due to dependence on the approval process | Faster, continuous iteration without waiting for approval |\nRisk Management | Early approval to lock in decisions and reduce uncertainty | Risk is managed through continuous control validation with automated evidence gathering, providing stronger assurance for second and third lines of defense than assessment at a specific point in time |\nCompliance | Manual checks by experts | Automated policy implementation via code and AI tools |\nTransparency | Limited, focused on approval documentation | High, lightweight decision record for technical stakeholders and visualizations or dashboards for non-technical monitoring functions |\nCollaboration | Centralized control, limited collaboration between teams | Peer-led communities (collective governance) like Security Guardians |\nInnovation | Limited, focused on adherence to approved design | Encouragement is encouraged for teams to explore within standards-based frameworks.\nDespite the benefits, many organizations struggle to let go of approval processes for a number of reasons, including:\n• Cultural Resistance – In risk-avoiding cultures where failure Fast-track approvals are unacceptable, and leaders hesitate to abandon centralized control mechanisms.\n• Compliance Concerns – In regulated industries, centralized approvals serve as a control gateway. Modern perspectives replace point-to-point trust with continuous compliance mechanisms—automated barriers, real-time monitoring, and evidence gathering—allowing even highly regulated environments to achieve compliance with small, autonomous teams operating within clearly defined boundaries (“two-pizza team”).\n• Lack of Infrastructure – Some organizations lack a self-service, automated compliance, or observational infrastructure, so they revert to the approval process to manage risk.\n• Governance Concerns – Traditional teams often perceive distributed decisions as lacking governance rather than being transformed governance.\nThe modern perspective offers significant benefits, albeit with governance considerations:\n• Speed ​​and Flexibility – Teams move faster without waiting for approvals, deploying AWS resources repeatedly.\n• Empowerment and Ownership – Builders using standards and ADRs feel accountable and actively shape the architecture.\n• Innovation and Experimentation – Self-service tools and AI guidance drive experimentation without delay.\nConclusion You can empower your builders by rethinking your architecture approval process. In the modern perspective discussed in this post, architecture governance aligns with the speed and flexibility of the cloud, allowing teams to innovate within a shared framework. This approach values ​​standards and autonomy over control, and transforms your architectural function into a strategic partner in a rapidly evolving landscape.\nTo learn how to set up and maintain cloud-centric principles and patterns, refer to the platform architecture chapter of the AWS Cloud Adoption Framework and the AWS Culture of Security resource.\nRelated Resources • When Security, Safety, and Urgency Are All Important: Handling Log4Shell (AWS re:Invent 2022)\n• Maintaining Visibility Over the Use of Cloud Architecture Patterns\n• Accelerating Deployments on AWS with Effective Governance\nAbout the Author ![Rostislav] Markov\nRostislav Markov\nRostislav is a principal architect with AWS Professional Services. As a technical leader in AWS Industries, he works with AWS customers and partners on their cloud transformation programs. Outside of work, he enjoys spending time outdoors with his family, playing tennis and skiing.\nTAGS: Architecture, AWS Well-Architected, Best Practices, Industries, Intermediate (200)\n"},{"uri":"https://tiendon.github.io/aws-worklog/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":"Choosing the Right Execution Options for Your SaaS Product in the AWS Marketplace Author:\nPawan Kumar, Technical Account Manager at Amazon Web Services\nDate: March 28, 2025\nAWS Marketplace sellers, independent software vendors (ISVs), and consulting partners (CPs) need to provide execution options when launching software-as-a-service (SaaS) products in the AWS Marketplace. Choosing the right execution option is crucial. This choice affects how customers access your product and can truly impact their experience. Let\u0026rsquo;s explore the available execution options to help you make an informed choice for your SaaS service.\nPrerequisites Before you choose an execution option, you must register as an AWS Marketplace seller. For more information, see Registering as an AWS Marketplace seller in the AWS Marketplace Seller Guide.\nExecution Options AWS Marketplace allows you to choose from two execution options when listing a SaaS product. The execution option defines the experience your customers have after registering your product in AWS Marketplace.\nDefault Execution URL: This option allows you to design and manage the entire onboarding experience. It works well for SaaS products that don\u0026rsquo;t require additional resources in the customer\u0026rsquo;s AWS account. The key details are: • Customers are redirected to your product registration page after signing up.\n• You, as the vendor, control the entire registration experience and provide customers with the steps to access the product.\nSaaS Quick Launch: This option works well if your SaaS product requires AWS resources deployed in the customer\u0026rsquo;s account. For example, vendors who deploy AWS Identity and Access Management roles, databases, or agents in the customer\u0026rsquo;s AWS account can simplify the onboarding experience for their customers. Key details are: • SaaS Quick Launch uses integrations to create the AWS CloudFormation stack to deploy resources with fewer context switches for customers.\n• Deployment secrets can be stored in the customer\u0026rsquo;s AWS Secrets Manager and used by AWS CloudFormation during deployment. This avoids the need for customers to copy and paste deployment parameters.\n• It provides step-by-step guidance including integrated authorization verification mechanisms to ensure customers have the necessary AWS credentials.\n• Products with SaaS Quick Launch display a Quick Launch tag in their descriptions.\nFor more information, see Enabling SaaS Quick Launch.\nConclusion and Next Steps In this post, we covered two execution options available for SaaS products in the AWS Marketplace. After choosing the execution option that best suits your needs, you can proceed with listing your SaaS in the AWS Marketplace. You may find the following resources helpful once you begin the listing process.\n• Practice Exercise: Creating a SaaS Listing\n• Practice Exercise: Integrating Your SaaS\n• Practice Exercise: Enabling SaaS Quick Launch\n• Successfully Testing Your SaaS Listing in AWS Marketplace\n• [Update Product Visibility] product\nAbout the Author Pawan Kumar is a Technical Account Manager at Amazon Web Services, specializing in AWS Marketplace solutions and serverless architecture. He develops innovative strategies to address complex customer challenges. He aims to drive cloud adoption across industries. Outside of work, Pawan enjoys playing cricket and following international tournaments.\nTAGS: AWS Marketplace, Best Practices, Foundational (100), SaaS, Software\n"},{"uri":"https://tiendon.github.io/aws-worklog/","title":"Internship Report","tags":[],"description":"","content":"Internship Report ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Vũ Tiến Đơn\nPhone Number: 0975893248\nEmail: donvtse183237@fpt.edu.vn\nUniversity: FPT University Ho Chi Minh City\nMajor: Software Engineer\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Understand the objectives of the AWS internship. Become familiar with the working environment. Learn an overview of Cloud \u0026amp; AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference 1 Internship orientation, meet mentor \u0026amp; team 08/09/2025 08/09/2025 2 Learn about the FCJ program, read policies 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 3 Review Cloud knowledge: IaaS/PaaS/SaaS, Scalability 10/09/2025 10/09/2025 4 Learn AWS Global Infrastructure 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 5 Overview of AWS Well-Architected Framework 12/09/2025 12/09/2025 6 Summarize knowledge — prepare for next week: AWS Services 13/09/2025 13/09/2025 Week 1 Achievements: Understood internship objectives and expectations. Gained overview of Cloud Computing \u0026amp; AWS Global Infrastructure. Learned the 6 pillars of the AWS Well-Architected Framework. Ready for Week 2: Basic AWS Services. "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting acquainted with AWS and basic AWS services\nWeek 2: Getting acquainted with basic AWS Services and reviewing networking.\nWeek 3: Getting acquainted with Docker and basic DevOps.\nWeek 4: IAM permissions and VPC configuration.\nWeek 5: Learning about front-end clunks and free domains from Git Education and learning basic React.\nWeek 6: Learning React.\nWeek 7: Learn about AWS Security Architecture Design.\nWeek 8: Learn about serverless.\nWeek 9: Learn about S3.\nWeek 10: Discuss and brainstorm for the project.\nWeek 11: Work on the project.\nWeek 12: Add additional features and prepare slides.\n"},{"uri":"https://tiendon.github.io/aws-worklog/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “Vietnam Cloud Day 2025 :Ho Chi Minh City Connect Edition for Builders” Event Objectives Overall Strategy: Updates AWS\u0026rsquo;s vision and strategy in Vietnam, driving innovation and digital growth.\nAI/GenAI: Introduces the role of Generative AI in shaping the future of software development and optimizing operations.\nModernization: Presents modern architectural models (Microservices, Serverless) and how to transform legacy applications.\nPractical Solutions: Provides practical tools and demos (such as Amazon Q) to help developers and architects apply them effectively.\nSpeakers Eric Yeo – Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Jaime Valles – Vice President, Commercial Sales \u0026amp; Business Development APJ, AWS Jeff Johnson – Managing Director ASEAN, AWS Dr Jens Lottner – Chief Executive Officer, Techcombank Trang Phung – CEO, U2U Network Vu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Dieter Botha – CEO, Tymex Nguyen Van Hai – Director of Software Engineering, Techcombank Nguyen Minh Nganh – AI Specialist, OCB Nguyen Manh Tuyen – Head of Data Application, LPBank Securities Key Highlights Technical Sessions by AWS Experts In-depth presentations delivered by AWS architects and industry experts, covering modern cloud architectures, security, scalability, and performance optimization. Real-World Use Cases Practical case studies demonstrating how organizations apply AWS services to solve real business and engineering challenges. Builder-Focused Content Sessions designed specifically for developers, DevOps engineers, and solution architects, with a strong emphasis on hands-on and implementation-driven topics. Community Networking Opportunities to connect with AWS professionals, local tech communities, and fellow builders to share knowledge and experiences. Key Takeaways Cloud Best Practices A clear understanding of AWS best practices for designing, building, and operating reliable, secure, and scalable systems. Modern Architecture Insights Knowledge of how to leverage containerization, serverless, and microservices architectures using AWS services. Security and Compliance Awareness Improved awareness of cloud security, identity management, and compliance strategies in production environments. Career and Skill Development Insights into the skills and learning paths needed to grow as a cloud engineer or solutions architect. Event Experience Hands-On Learning Environment A practical and interactive atmosphere that encourages learning through real examples, demos, and architectural discussions. Professional and Collaborative Atmosphere A well-organized event fostering open discussion, collaboration, and knowledge sharing among participants and speakers. Local Builder Community Engagement A strong sense of community among builders in Ho Chi Minh City, making the event accessible and relevant to local engineers. Inspiration and Motivation A motivating experience that inspired attendees to explore cloud technologies further and apply AWS solutions to their own projects. "},{"uri":"https://tiendon.github.io/aws-worklog/2-proposal/","title":"Proposal","tags":[],"description":"","content":"SportShop E-Commerce Platform A Cost-Optimized AWS Three-Tier Architecture for Online Sports Retail 1. Executive Summary The SportShop E-Commerce Platform is designed to modernize online retail operations using a simplified and cost-optimized AWS architecture, suitable for student projects and small-scale applications. The system uses ReactJS for the frontend, Spring Boot for the backend, and Amazon RDS MySQL for persistent data storage.\nTo reduce cost and complexity, the architecture removes Amazon Cognito, the Application Load Balancer (ALB), NAT Gateway, and banking payment integration. User authentication is handled directly by the backend, which runs on Elastic Beanstalk single-instance mode, while static content is delivered through S3 + CloudFront.\nDespite being simplified, the architecture still follows AWS best practices for security and CI/CD using ACM, Parameter Store, CloudWatch, CodePipeline, and CodeBuild.\n2. Problem Statement Current Problems Slow, manual, and inconsistent deployment workflows Risk of leaked or improperly stored credentials Limited system monitoring and alerting High chance of downtime during updates Proposed Solution The platform leverages AWS managed services to automate deployments, enhance security, and implement a clean three-layer architecture:\nEdge: Route 53, CloudFront (ACM) Application: Elastic Beanstalk (single-instance) Data: RDS MySQL, S3, CloudWatch Secrets such as database credentials and JWT signing keys are securely stored in Parameter Store.\nCI/CD is managed through GitLab → CodePipeline → CodeBuild.\nBenefits Lower AWS cost and reduced architectural complexity Faster deployments through automation Global HTTPS delivery through CloudFront Clear separation between frontend, backend, and database layers 3. Solution Architecture AWS Services Used Amazon Route 53 — DNS Amazon CloudFront — CDN and HTTPS distribution AWS Certificate Manager — SSL/TLS certificates Elastic Beanstalk (Single Instance) — Backend hosting Amazon RDS MySQL — Main database Amazon S3 — Static hosting \u0026amp; media storage Parameter Store — Secure secret management CloudWatch — Logs and metrics CodePipeline + CodeBuild — CI/CD automation Layered Architecture Edge Layer\nRoute 53 → CloudFront CloudFront serves the ReactJS frontend globally over HTTPS Application Layer\nSpring Boot backend deployed on Elastic Beanstalk single-instance Authentication handled with JWT Secure configuration loaded from Parameter Store Data Layer\nRDS MySQL stores structured application data S3 stores static content and media files CloudWatch provides centralized monitoring 4. Technical Implementation Configure VPC, subnets, and security groups Host frontend on S3 + CloudFront Deploy backend with Elastic Beanstalk Provision RDS MySQL Store secrets in Parameter Store CI/CD pipeline: GitLab → CodePipeline → CodeBuild Enable monitoring via CloudWatch Tech Stack:\nFrontend: ReactJS (S3 + CloudFront) Backend: Spring Boot (Java 17) Database: Amazon RDS MySQL CI/CD: AWS CodePipeline, CodeBuild Security: IAM, ACM, Parameter Store, CloudWatch 5. Timeline \u0026amp; Milestones Month 1 Set up AWS environment (VPC, RDS, S3, CloudFront, Route 53) Deploy backend \u0026amp; frontend Configure CI/CD pipeline Month 2 Implement JWT authentication Develop modules: product, order, user management Set up monitoring \u0026amp; alerting Month 3 Testing and optimization Final deployment Documentation and training 6. Budget Estimation You can see the cost on the AWS Pricing Calculator Or download the budget estimate file.\nAWS Service Monthly Cost Amazon RDS MySQL $21.84 Elastic Beanstalk (EC2 t3.micro) $11.68 Amazon S3 $0.26 Data Transfer $0.60 CloudFront $0.88 Route 53 $0.90 ACM Public Certificate $0 CloudWatch Metrics $3.02 CodePipeline $0 CodeBuild $0.50 Parameter Store $0 CloudWatch Logs $1.41 Estimated total: ~$35–40 per month\nYearly: ~$420–480\n7. Risk Assessment Risks Single-instance backend → No fault tolerance No NAT Gateway → Backend cannot call external APIs Risk of JWT token misuse without proper security Mitigation Enable Elastic Beanstalk auto-recovery and health checks Apply secure authentication \u0026amp; token handling practices Utilize RDS automated backups and snapshots 8. Future Enhancements Amazon Cognito — MFA, OAuth, Social Login Application Load Balancer — Multi-instance scaling \u0026amp; high availability NAT Gateway — Allow backend to call external APIs Payment Integrations: VNPay, Stripe, PayPal RDS Multi-AZ — High availability and automatic failover S3 Lifecycle + Glacier — Cost-optimized long-term storage AWS X-Ray + OpenSearch — Tracing and advanced logging Microservices Architecture with ECS/EKS — Scale backend into multiple services "},{"uri":"https://tiendon.github.io/aws-worklog/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AI-Driven Development Life Cycle: Reimagining Software Engineering.” Event Objectives Introduce AI-Driven Development Concepts To help developers understand the fundamentals of AI-driven development and how generative AI can be embedded across the entire software development life cycle. Showcase AI Tools for Developers To demonstrate how Amazon Q Developer and Kiro can support developers in architecture design, coding, testing, deployment, and maintenance. Improve Developer Productivity To highlight how AI-powered automation reduces repetitive and undifferentiated tasks, enabling developers to focus on higher-value engineering work. Prepare Developers for the Future To equip participants with future-ready skills and insights needed to adapt to the evolving landscape of AI-enabled software engineering. Speakers Toan Huynh My Nguyen Key Highlights AI-Driven Development Life Cycle Overview A comprehensive introduction to how generative AI is transforming the entire software development life cycle, from system architecture to maintenance. Amazon Q Developer Demonstration A live demonstration showcasing how Amazon Q Developer assists developers in coding, testing, troubleshooting, and accelerating everyday development tasks. Kiro Demonstration An interactive demo illustrating how Kiro can be used to streamline development workflows and enhance developer productivity through AI-driven automation. Expert-Led Session Knowledge sharing led by experienced AWS instructors, providing practical insights and real-world perspectives on AI-powered software engineering. Key Takeaways Understanding Generative AI in Software Engineering A clear understanding of how generative AI reshapes the way applications are designed, built, deployed, and managed securely. Increased Developer Productivity Insights into how AI tools automate undifferentiated heavy-lifting tasks, allowing developers to focus on higher-value and creative work. Practical AI Tool Adoption Hands-on knowledge of integrating Amazon Q Developer and Kiro into daily development workflows. Future-Ready Development Skills Key learnings on building AI-ready skills that prepare developers and teams for the future of software engineering. Event Experience Engaging Technical Session A well-structured session combining theory and live demonstrations to ensure both conceptual understanding and practical exposure. Collaborative Learning Environment An open and interactive atmosphere encouraging discussions, questions, and knowledge sharing among participants. Professional AWS Venue A premium learning experience hosted at the AWS Event Hall, providing a professional and focused environment for deep technical learning. Inspiration to Adopt AI-Driven Development A motivating experience that encouraged attendees to explore and adopt AI-driven tools in their own software development life cycles. "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Familiarize with basic AWS Services. Utilize the Console \u0026amp; CLI. Review networking fundamentals. Tasks: Day Task Start Date Completion Date Reference 1 Familiarization with the FCJ group, reading requirements 09/15/2025 09/15/2025 2 Studying AWS service groups: Compute, Storage, Creating AWS Free Tier Account 09/16/2025 09/16/2025 3 Practicing AWS CLI, Learning basic EC2 09/17/2025 09/17/2025 4 Practicing EC2 SSH + EBS 09/18/2025 09/18/2025 5 Reviewing networking fundamentals 09/19/2025 09/19/2025 https://youtu.be/IPvYjXCsTg8?si=u4uiBRpLU3zeGf-0 6 Reviewing networking fundamentals 09/20/2025 09/20/2025 Achievements: Created and configured an AWS account. Used the AWS CLI \u0026amp; Console. Learned to launch EC2 and manage EBS. "},{"uri":"https://tiendon.github.io/aws-worklog/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Modernizing your application isn’t difficult: Migrating to Amazon EKS with your existing NLB configuration This blog introduces how to modernize your existing application by migrating it to Amazon Elastic Kubernetes Service (Amazon EKS) while preserving your current Network Load Balancer (NLB) configuration. You will learn why EKS provides a more scalable, reliable, and manageable platform for containerized workloads, and how maintaining your existing NLB setup ensures a smooth transition with minimal architectural changes. The article also guides you through key migration steps, networking considerations, and best practices to help your team adopt Kubernetes without disrupting your current production environment.\nBlog 2 - Enhance LLM observability with Amazon Bedrock and Dynatrace This blog introduces how organizations can enhance Large Language Model (LLM) observability by integrating Amazon Bedrock with Dynatrace. As generative AI workloads become more complex and widely adopted, gaining deeper insights into model performance, cost, latency, and user interactions is crucial for maintaining reliability. This blog explores practical approaches, architectural considerations, and monitoring techniques to help teams achieve end-to-end visibility across LLM applications.\nBlog 3 - Empower Your Teams with Modern Architecture Governance This blog introduces how modern architecture governance can empower engineering teams to innovate faster while maintaining security, compliance, and operational excellence. As organizations scale their cloud environments, establishing a well-defined governance model becomes essential to keep architectures consistent, cost-efficient, and aligned with business goals. This blog explores key principles, practical governance frameworks, and actionable strategies that help teams balance autonomy with control in a modern cloud-native environment.\nBlog 4 - Choosing the Right Execution Options for Your SaaS Product in the AWS Marketplace This blog introduces the key execution options available for delivering your SaaS product through the AWS Marketplace, helping you make informed decisions on how to package, deploy, and operate your solution. You will learn how different delivery models—such as SaaS Contracts, SaaS Subscriptions, and Usage-Based Pricing—impact customer onboarding, billing, scalability, and operational complexity. This blog also highlights important architectural and business considerations to ensure your SaaS offering is optimized for both customer experience and long-term growth in the AWS ecosystem.\n"},{"uri":"https://tiendon.github.io/aws-worklog/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: “Discover Agentic AI: Amazon QuickSuite Workshop” Event Objectives Introduce Agentic AI Concepts To help participants understand the concept of agentic AI and how AI agents can reason, act, and automate tasks across enterprise workflows. Showcase Amazon QuickSuite To demonstrate how Amazon QuickSuite enables AI-powered collaboration by connecting data, insights, and actions within a unified workspace. Enable Business Productivity with AI To illustrate how organizations can use agentic AI to reduce manual effort, streamline workflows, and accelerate decision-making. Support Practical AI Adoption To provide practical exposure that helps teams confidently evaluate and adopt AI-driven solutions for real business scenarios. Speakers Vivien Nguyen: Territory Manager, AWS Tung Cao: Solution Architect, AWS Khang Nguyen: Solution Architect, Cloud Kinetics Key Highlights Hands-on workshop building Agentic AI concepts directly with AWS technical experts Earliest access opportunity to the potential and optimal application methods of Amazon Quick Suite Real-world application of Agentic AI concepts to your specific use cases Direct access to AWS\u0026rsquo;s latest innovations and partner solutions Key Takeaways Clear Understanding of Agentic AI Participants gain a solid understanding of how agentic AI differs from traditional AI and how it drives autonomous, goal-oriented workflows. Faster Decision-Making QuickSuite enables faster insights and responses by reducing the time spent searching for data and coordinating across systems. Democratized Access to Intelligence Both technical and non-technical users can leverage natural language interaction to extract insights and automate tasks. Scalable AI-Driven Workflows Attendees learn how AI-powered workflows can scale across teams and departments, supporting long-term digital transformation. Event Experience Interactive Workshop Format A hands-on and demo-driven workshop that helps participants see AI concepts applied in realistic business use cases. Practical and Business-Focused Examples and demonstrations are grounded in real-world scenarios, making the learning experience immediately relevant. Inclusive Learning Environment The session is accessible to a broad audience, including business users, engineers, and decision-makers. Inspiring Future-Oriented Mindset The workshop encourages participants to rethink traditional workflows and embrace AI-driven ways of working. "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Get familiar with Docker and basic devops Tasks: Day Task Start Date Completion Date Reference 1 Learning Docker 09/15/2025 09/15/2025 https://youtu.be/GFgJkfScVNU?si=DzO7r275vnjkvO1e 2 Further research on Docker and Terraform 09/16/2025 09/16/2025 https://youtu.be/ENr2QvcHrL4?si=I_iRFaGgQ2x2yxdY, https://youtu.be/UoqoU4IJ3No?si=XcCo0WNc2UFk4ZAL 3 Learning about basic DevOps 09/17/2025 09/18/2025 https://youtu.be/sSRaakd95Nk?si=-upMivhoej5cWRs7 4 Learning about basic DevOps (2) 09/17/2025 09/18/2025 https://youtu.be/sSRaakd95Nk?si=-upMivhoej5cWRs7 5 Consulting and learning about an AWS application project 09/19/2025 09/19/2025 https://youtu.be/KoY6fS77pDc?si=pSrnU53XQXwTCnDl 6 Consulting and learning about an AWS application project 09/20/2025 09/20/2025 https://youtu.be/KoY6fS77pDc?si=pSrnU53XQXwTCnDl Achievements: Created and configured an AWS account. Used the AWS CLI \u0026amp; Console. Learned to launch EC2 and manage EBS. "},{"uri":"https://tiendon.github.io/aws-worklog/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://tiendon.github.io/aws-worklog/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: “AWS Cloud Mastery Series #1” Event Objectives Introduce AWS AI/ML and Generative AI To provide participants with a clear understanding of AWS AI/ML and generative AI services, including their roles in modern application development. Build Practical AI Knowledge To equip attendees with foundational and practical skills to design, build, and deploy AI/ML and generative AI solutions on AWS. Encourage Hands-On Learning To enable participants to experience real-world AI workflows through demonstrations and guided examples. Strengthen Community and Knowledge Sharing To foster collaboration and networking within the local AWS and AI/ML community. Speakers Bao Huynh – AWS Community Builder Thinh Nguyen – AWS Community Builder Vi Tran – AWS Community Builder Key Highlights Overview of AWS AI/ML Ecosystem A comprehensive introduction to AWS AI/ML services, covering model development, training, deployment, and scaling. SageMaker Studio Demonstration A live walkthrough of SageMaker Studio, showcasing end-to-end machine learning workflows on AWS. Generative AI with Amazon Bedrock An introduction to generative AI concepts, prompt engineering techniques, and building chatbots using Amazon Bedrock. Knowledge Base and AI Agents Exploration of retrieval-augmented generation (RAG) and AI agents for handling multi-step and knowledge-driven tasks. Key Takeaways Solid Foundation in AWS AI/ML Participants gain a strong understanding of how AI/ML and generative AI workloads are implemented on AWS. Hands-On Generative AI Skills Practical exposure to prompt engineering, AI model interaction, and building generative AI applications. Readiness for Real-World AI Projects Confidence to start or contribute to AI/ML and GenAI projects using AWS-managed services. Awareness of AI Best Practices Understanding of responsible AI usage, scalability, and security considerations. Event Experience Interactive and Practical Sessions A demo-driven learning experience that connects theory with real-world implementation. Supportive Learning Environment A collaborative atmosphere encouraging questions, discussion, and peer learning. Community-Centered Event Strong engagement with the local tech community, fostering knowledge exchange and networking. Motivating and Forward-Looking An inspiring experience that motivates participants to further explore AWS AI/ML and generative AI technologies. "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: IAM Permissions management and VPC configuration. Weekly Tasks: Day Task Start End Reference 1 Learning AWS IAM fundamentals 09/28/2025 09/28/2025 https://000002.awsstudygroup.com/ 2 Exploring VPC, Subnets, Internet Gateway, and Route Table 09/29/2025 09/29/2025 https://000003.awsstudygroup.com/ 3 Learning about NAT Gateway and VPC Firewalls (Security Groups and Network ACLs) 09/30/2025 09/30/2025 https://000003.awsstudygroup.com/ 4 VPC Workshop Setup (Hands-on configuration) 10/01/2025 10/01/2025 https://000003.awsstudygroup.com/ 5 Deploying Amazon EC2 Instances within the custom VPC 10/02/2025 10/02/2025 https://000003.awsstudygroup.com/ 6 Summary and Review of VPC configuration 10/03/2025 10/03/2025 Achievements: Successfully initialized and configured an AWS VPC. Deep understanding of IAM core concepts including Users, Groups, Roles, and Policies. Gained practical knowledge on creating and attaching IAM Policies to enforce Least Privilege Access. Understood the crucial role of IAM Roles in securely granting permissions to AWS services (e.g., EC2 Role). Successfully set up fundamental VPC components: Subnets, Internet Gateway, Route Tables, NAT Gateway, Security Groups (SG), and Network ACLs (NACL). "},{"uri":"https://tiendon.github.io/aws-worklog/5-workshop/","title":"Workshop","tags":[],"description":"","content":""},{"uri":"https://tiendon.github.io/aws-worklog/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: “Cloud Mastery Series #2: DevOps on AWS” Event Objectives Deepen DevOps \u0026amp; Cloud Engineering Knowledge To provide participants with a thorough understanding of DevOps practices and cloud-native infrastructure using AWS tools and services. To help bridge the gap between traditional development workflows and modern DevOps culture including automation, CI/CD, containerization, and infrastructure as code. Provide Hands-On Exposure to AWS DevOps Tools To demonstrate real AWS DevOps services: CI/CD pipelines, container services, Infrastructure as Code (IaC), monitoring \u0026amp; observability. To let participants experience live demos and workflows from coding to deployment and operations on AWS. Empower Engineers to Build Scalable, Reliable Systems To equip attendees with the knowledge and best practices required to design, deploy, and maintain scalable, maintainable, and observable systems in production. To enable participants to adopt DevOps mindset and practices, improving deployment frequency, stability, scalability, and team collaboration. Support Career Growth \u0026amp; Cloud Fluency To support participants’ journey towards becoming more competent cloud engineers or DevOps practitioners. To provide clarity on AWS certifications path and DevOps career pathways in cloud environments. Speakers Key Highlights Challenges of Manual Infrastructure (“ClickOps”) Deployments take time, causing delays. Prone to manual errors and inconsistent configurations across environments. Difficult for teams to collaborate and maintain reproducible setups. Introduction to Infrastructure as Code (IaC) Manage cloud resources through code rather than manual console operations. Benefits: automation, scalability, reproducibility, and reduced provisioning errors. AWS CloudFormation AWS native IaC service using YAML or JSON templates to define and deploy stacks. Automatically creates, updates, and deletes resources as a unified unit. Supports drift detection to identify and fix deviations from templates. AWS Cloud Development Kit (CDK) Framework for defining IaC using familiar programming languages like Python, TypeScript, or Java. Constructs at multiple levels: L1 (direct CloudFormation mapping), L2 (developer-friendly defaults), L3 (pre-built patterns). CLI for initializing projects, synthesizing templates, deploying stacks, and detecting drifts. Choosing IaC Tools Consider single vs. multi-cloud, team expertise (developers vs. operations), and ecosystem compatibility. Comparison with Terraform, which uses HCL for cross-provider support. Docker and Containers Containers are lightweight, portable units including applications and dependencies. Dockerfiles ensure consistent images across development, testing, and production. Amazon ECR for secure image storage, scanning, and management. AWS Container Orchestration Services Amazon ECS: managed Docker orchestration with EC2 or Fargate. Amazon EKS: Kubernetes clusters with auto-scaling and updates. AWS App Runner: serverless deployment of web apps from code or images. Comparison: ECS is simpler and AWS-integrated; EKS offers flexibility and Kubernetes standards. CI/CD on AWS Source control with CodeCommit, using GitFlow or trunk-based development. Build \u0026amp; test with CodeBuild, deployment via CodeDeploy (blue/green, canary, rolling). Orchestration with CodePipeline for end-to-end automation. Demo showing a full pipeline from commit to production. Monitoring and Observability AWS CloudWatch for metrics, logs, dashboards, and alerts. AWS X-Ray for request tracing and identifying bottlenecks. Best practices: alerting, on-call rotations, full-stack visibility. DevOps Best Practices \u0026amp; Case Studies Advanced strategies: feature flags, A/B testing, automated testing integration. Incident management: blameless postmortems, lessons from startups and enterprises. Key Takeaways DevOps Mindset Promote collaboration between development and operations to accelerate delivery. Track metrics like DORA to measure deployment speed and reliability. Foster a culture of continuous improvement based on business needs. Technical Practices Use IaC for automated, version-controlled infrastructure to prevent errors. Leverage CDK constructs for reusable patterns in complex setups. Implement container orchestration with ECS or EKS according to workloads and team skills. Build robust CI/CD pipelines for frequent, low-risk releases. Observability \u0026amp; Reliability Integrate CloudWatch and X-Ray for proactive monitoring and rapid troubleshooting. Use deployment strategies like canary releases to minimize downtime. Perform regular drift checks and apply lifecycle policies for container management. Modernization Approach Assess current setups before migrating to microservices or containers. Implement in phases, experimenting with tools like App Runner for quick wins. Measure outcomes through cost savings, faster iterations, and improved agility. Applying to Work Incorporate Git strategies and CodePipeline into workflows for better automation. Try CDK for IaC in pilot projects to improve developer productivity. Set up observability dashboards to enhance incident response times. Conduct event storming to model container-based architectures. Pursue AWS DevOps certifications to advance career paths. Event Experience Learning from experts: AWS Community Builders shared practical knowledge, demos, and examples of microservices deployments. Hands-on technical learning: Explored CI/CD pipelines, IaC, and container orchestration; learned ECS vs. EKS trade-offs; monitored and remediated configuration drift. Effective use of AWS tools: CDK simplifies IaC using code, App Runner supports rapid deployment, and observability tools optimize application performance. Networking \u0026amp; interaction: Q\u0026amp;A and group discussions allowed exchanging ideas about tools and career guidance, aligning technology with business goals. Lessons Learned Automating IaC and CI/CD reduces risks and improves efficiency. Balanced container orchestration ensures scalability without complexity. Observability and blameless postmortems foster a resilient DevOps culture. "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Research CloudFront and obtain a free domain through GitHub Education. Learn basic React through a practical project. Tasks: Day Task Start End Reference 1 Researching and learning about CloudFront 10/06/2025 10/06/2025 https://youtu.be/7anq53SGxBk?si=zaAFYprDLB8LgUog 2 Utilizing the free GitHub domain for a deployment 10/07/2025 10/07/2025 3 Learning React fundamentals 10/08/2025 10/08/2025 https://youtu.be/r47C9c4qCqE?si=AjyCvvUPOhp_ptaP 4 Continuing React study 10/09/2025 10/09/2025 https://youtu.be/r47C9c4qCqE?si=AjyCvvUPOhp_ptaP 5 Continuing React study 10/10/2025 10/10/2025 https://youtu.be/r47C9c4qCqE?si=AjyCvvUPOhp_ptaP 6 Continuing React study "},{"uri":"https://tiendon.github.io/aws-worklog/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Throughout my work and studies, I have always strived to do my best. However, I realized that during my internship at Amazon Web Service (AWS), due to my own shortcomings in self-discipline and responsibility, I sometimes failed to complete my studies and work effectively, leading to unwanted procrastination and distractions.\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ☐ ✅ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Enhance self-study ability, discipline in learning and researching Build reasonable working time, have a delay deadline Upgrade the next ability and problem solving and improve foreign languages "},{"uri":"https://tiendon.github.io/aws-worklog/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"Summary Report: “Cloud Mastery Series #3: Security on AWS” Event Objectives Provide Foundational Knowledge of AWS Security Pillars Introduce the five pillars of the AWS Well-Architected Security Framework and explain the role of the Security Pillar in designing secure and resilient cloud architectures. Strengthen Cloud Security Skills Equip participants with essential knowledge to identify risks, build multi-layered protection, and implement industry-standard security measures on AWS. Improve Practical Understanding Through Demonstrations Help attendees gain hands-on awareness through demos such as IAM policy simulation, threat detection scenarios, incident handling workflows, and resource protection techniques. Foster Networking and Knowledge Sharing Encourage collaboration and knowledge exchange within the local AWS engineering community in Vietnam. Speakers Kha Van (AWS) – Host / Facilitator Tran Duc Anh: Cloud Security Engineer Trainee, AWS Cloud Club Captain SGU Nguyen Tuan Thinh: Cloud Engineer trainee Nguyen Do Thanh Dat: Cloud engineer Trainee Thinh Lam: FCJer Viet Nguyen: FCJer Key Highlights Shared Responsibility Model \u0026amp; Security Foundations Clarified the division of responsibilities between AWS and customers. Introduced core security principles: Least Privilege, Zero Trust, and Defense in Depth. Discussed common cloud vulnerabilities and threat patterns observed in Vietnam. Identity \u0026amp; Access Management (IAM) Covered IAM Users, Roles, Policies, and AWS security best practices. Demonstrated AWS IAM Identity Center (SSO), permission sets, and centralized identity management. Emphasized MFA enforcement, credential rotation, and IAM policy validation using simulation tools. Detection \u0026amp; Monitoring CloudTrail at the Organization level for unified logging. GuardDuty for threat intelligence and behavior anomaly detection. Security Hub for centralized security posture checks based on CIS and AWS best practices. EventBridge used for automating alerting and remediation actions. Infrastructure Protection VPC security: segmentation, subnet isolation, Security Groups vs NACLs. Use of AWS WAF, AWS Shield, and AWS Network Firewall. Workload protection: EC2 hardening, container image scanning, and runtime threat detection. Data Protection Encryption at-rest and in-transit for S3, EBS, RDS, and other AWS services. AWS KMS: key policies, key rotation, grants, and permission boundaries. Secrets Manager and Parameter Store for secure secrets lifecycle management. Data classification, access guardrails, and governance controls. Incident Response Explained the AWS incident response lifecycle and best practices. Reviewed multiple real-world playbooks: Compromised IAM credentials Publicly exposed S3 buckets Malware detection in cloud workloads Showed workflows to snapshot, isolate, collect logs, and recover workloads. Demonstrated automated remediation using Lambda and Step Functions. Key Takeaways Security Mindset Security is a continuous process, not a one-time setup. Apply least privilege consistently and enforce role-based access. Build security into design stages, not after deployment. Cloud Security Best Practices Enable mandatory logging: CloudTrail Organization, GuardDuty. Prefer IAM Roles over long-term access keys. Enforce encryption-by-default across all services. Perform configuration drift detection to maintain consistency. Observability \u0026amp; Threat Detection Combine CloudWatch, Security Hub, and GuardDuty for deep visibility. Use severity-based alerting integrated with Slack or email. Implement distributed tracing with X-Ray for microservices troubleshooting. Incident Preparedness Standardize incident playbooks and runbooks. Automate security responses for high-risk events. Conduct periodic security game days to practice real scenarios. Applying to Work Reassess current environments for Well-Architected compliance. Establish baseline security for IAM, logging, networking, and encryption. Deploy Security Hub and GuardDuty across multi-account environments. Integrate scanning into CI/CD and container workflows. Explore the AWS Security Specialty certification. Event Experience Learning from AWS Experts:\nAttendees benefited from real-world insights, best practices, and practical demos delivered by AWS engineers and Community Builders.\nHands-on Learning:\nParticipants observed live simulations of IAM access checks, organization-wide logging setups, GuardDuty threat findings, and incident response flows.\nEffective Use of AWS Security Tools:\nTools like CloudTrail, GuardDuty, Security Hub, KMS, and WAF demonstrated how to build continuous protection and monitoring capabilities.\nNetworking \u0026amp; Community Interaction:\nQ\u0026amp;A sessions and group discussions allowed participants to share experiences, gain career insights, and align technical decisions with business goals.\nLessons Learned Multi-layered security ensures resilience even if one layer fails. Logging and monitoring are essential, not optional. Automation reduces human error and accelerates response time. Blameless postmortems and open communication help teams improve continuously. "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Deepen knowledge and skills in React. Tasks: Day Task Start End Reference 1 Implementing light/dark mode theme switching with React 10/13/2025 10/13/2025 https://youtu.be/ChjAOpW28ys?si=GHCW1iDuSXrHjdJ- 2 Learning Tailwind CSS fundamentals 10/14/2025 10/14/2025 3 Exploring UI component libraries such as shadcn/ui 10/15/2025 10/15/2025 4 Researching and learning about Zustand (React state management) 10/16/2025 10/17/2025 5 Continuing Zustand research and implementation 10/16/2025 10/17/2025 6 Summary and Review of weekly progress 10/18/2025 10/18/2025 Achievements: Achieved a deeper understanding and practical skills in React development. Learned to integrate modern styling frameworks like Tailwind CSS. Understood the concept and application of UI component libraries. Explored and started learning a modern state management solution, Zustand. "},{"uri":"https://tiendon.github.io/aws-worklog/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment\nThe working environment is friendly, positive, and supportive. FCJ members are enthusiastic and always willing to help whenever I have questions or need guidance. The workspace is comfortable, clean, and well-organized, which creates a pleasant atmosphere for learning and working. In the future, it would be great to have more external activities or team-building events to strengthen connections among members.\n2. Learning \u0026amp; Skill Development Opportunities\nThroughout my internship, I had many opportunities to learn valuable services and tools while developing a more disciplined and professional working style. I became familiar with the workplace environment and significantly improved my communication and teamwork skills. In addition, I had the chance to attend sessions led by experienced instructors and join knowledge-sharing events, which provided practical insights and helped me better orient my career direction.\n3. Internship Policies / Benefits\nThe company offers flexible working hours, along with opportunities to participate in training sessions and technical events. These activities allow interns to learn from talented and passionate professionals who are always willing to share their knowledge, answer questions, and support interns’ learning journeys.\n4. Mentor and Support Team\nThe mentor is highly enthusiastic, dedicated, and experienced. The support team is always approachable and ready to assist, creating a comfortable and encouraging working environment. They actively help resolve challenges and provide clear guidance when interns face difficulties or complex issues.\nAdditional Questions What did you find most satisfying during your internship?\nThe positive working environment, access to modern services and tools, and meaningful knowledge- and experience-sharing sessions. If recommending to a friend, would you suggest they intern here? Why or why not?\nYes, I would strongly recommend this internship. I would encourage my friends to be confident in communication, proactive in learning, and effective in managing their own time. The company provides many learning opportunities, valuable knowledge, and a supportive team of passionate professionals. Interns will be positively surprised by both the working environment and the guidance from instructors. "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: To advance Frontend Development skills (React) and learn Backend Deployment on AWS.\nResearch and apply React Query for Server-Side state management. Practice deploying a Node.js application onto AWS Elastic Beanstalk (EB). Review and apply basic security principles in architecture (IAM, SG, Encryption). Detailed Weekly Tasks: Day Task Start End Reference 1 React Frontend: Researching and applying React Query (TanStack Query) for data Fetching and Caching 10/20/2025 10/20/2025 2 Backend Deployment: Learning about Elastic Beanstalk (EB) and structuring a Node.js application for deployment 10/21/2025 10/21/2025 https://000112.awsstudygroup.com/ 3 EB Practice: Successfully deploying a Node.js project (or template) onto the EB environment 10/22/2025 10/22/2025 https://000112.awsstudygroup.com/ 4 Security Review (IAM/Encryption): Reviewing IAM Policies, Roles, and how to use KMS for encryption 10/23/2025 10/23/2025 5 Security Review (Networking): Reviewing Security Groups, NACLs, and researching WAF (Web Application Firewall) 10/24/2025 10/24/2025 6 Summary \u0026amp; Optimization: Testing, optimizing the deployed application on EB, and writing the weekly report 10/25/2025 10/25/2025 Achievements: React: Mastered the use of React Query for efficient data management and reduced State code. Deployment: Successfully deployed a Node.js application onto Elastic Beanstalk. Operations: Understood how EB manages environments, automates scaling, and handles application updates. Basic Security: Reviewed and applied IAM and Network Security principles (Security Groups) for the EB environment. "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Learn serverless concepts. Practice using AWS Lambda and API Gateway. Tasks: Day Task Start End Reference 1 Learn Serverless Architecture 27/10/2025 27/10/2025 2 Learn AWS Lambda fundamentals 28/10/2025 28/10/2025 3 Practice Lambda functions 29/10/2025 29/10/2025 4 Learn API Gateway 30/10/2025 30/10/2025 5 Integrate Lambda + API Gateway 31/10/2025 31/10/2025 6 Summary 01/11/2025 01/11/2025 Achievements: Built a serverless API with Lambda and API Gateway. "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Learn S3 advanced. Practice S3 static website hosting. Tasks: Day Task Start End Reference 1 S3 storage classes 03/11/2025 03/11/2025 2 Lifecycle rules 04/11/2025 04/11/2025 3 S3 versioning 05/11/2025 05/11/2025 4 S3 static website hosting 06/11/2025 06/11/2025 5 Configure CloudFront for S3 07/11/2025 07/11/2025 6 Summary 08/11/2025 08/11/2025 Achievements: Hosted a static website on S3 + CloudFront. "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Discussion and brainstorming for the final project. Tasks: Day Task Start End Reference 1 Define project scope \u0026amp; requirements 10/11/2025 10/11/2025 2 Define project scope \u0026amp; requirements 11/11/2025 11/11/2025 3 Design architecture (S3/EC2/RDS/Lambda…) 12/11/2025 12/11/2025 4 Design architecture (S3/EC2/RDS/Lambda…) 13/11/2025 13/11/2025 5 Design architecture (S3/EC2/RDS/Lambda…) 14/11/2025 14/11/2025 6 Design architecture (S3/EC2/RDS/Lambda…) 15/11/2025 15/11/2025 Achievements: "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Build project Tasks: Day Task Start End Reference 1 Build project 17/11/2025 17/11/2025 2 Build project 18/11/2025 18/11/2025 3 Build project 19/11/2025 19/11/2025 4 Build project 20/11/2025 20/11/2025 5 Build project 21/11/2025 21/11/2025 6 Build project 22/11/2025 22/11/2025 Achievements: Complete key features "},{"uri":"https://tiendon.github.io/aws-worklog/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Add extra features and prepare slides Tasks: Day Task Start End Reference 1 Build project 24/11/2025 24/11/2025 2 Build project 25/11/2025 25/11/2025 3 Build project 26/11/2025 26/11/2025 4 Build project 27/11/2025 27/11/2025 5 Build project \u0026amp; prepare slides 28/11/2025 28/11/2025 6 Build project 29/11/2025 29/11/2025 Achievements: "},{"uri":"https://tiendon.github.io/aws-worklog/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://tiendon.github.io/aws-worklog/tags/","title":"Tags","tags":[],"description":"","content":""}]