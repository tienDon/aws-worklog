<!doctype html><html lang=en class="js csstransforms3d"><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.134.3"><meta name=description content><meta name=author content="thienlh@thienlu.com"><link rel=icon href=https://tiendon.github.io/aws-worklog/images/favicon.png type=image/png><title>Blog 2 :: Internship Report</title>
<link href=https://tiendon.github.io/aws-worklog/css/nucleus.css?1765436011 rel=stylesheet><link href=https://tiendon.github.io/aws-worklog/css/fontawesome-all.min.css?1765436011 rel=stylesheet><link href=https://tiendon.github.io/aws-worklog/css/hybrid.css?1765436011 rel=stylesheet><link href=https://tiendon.github.io/aws-worklog/css/featherlight.min.css?1765436011 rel=stylesheet><link href=https://tiendon.github.io/aws-worklog/css/perfect-scrollbar.min.css?1765436011 rel=stylesheet><link href=https://tiendon.github.io/aws-worklog/css/auto-complete.css?1765436011 rel=stylesheet><link href=https://tiendon.github.io/aws-worklog/css/atom-one-dark-reasonable.css?1765436011 rel=stylesheet><link href=https://tiendon.github.io/aws-worklog/css/theme.css?1765436011 rel=stylesheet><link href=https://tiendon.github.io/aws-worklog/css/hugo-theme.css?1765436011 rel=stylesheet><link href=https://tiendon.github.io/aws-worklog/css/theme-workshop.css?1765436011 rel=stylesheet><script src=https://tiendon.github.io/aws-worklog/js/jquery-3.3.1.min.js?1765436011></script><style>:root #header+#content>#left>#rlblock_left{display:none!important}</style></head><body data-url=https://tiendon.github.io/aws-worklog/3-blogstranslated/3.2-blog2/><nav id=sidebar class=showVisitedLinks><div id=header-wrapper><div id=header><a id=logo href=https://tiendon.github.io/aws-worklog/><svg id="Layer_1" data-name="Layer 1" viewBox="0 0 60 30" width="30%"><defs><style>.cls-1{fill:#fff}.cls-2{fill:#f90;fill-rule:evenodd}</style></defs><title>AWS-Logo_White-Color</title><path class="cls-1" d="M14.09 10.85a4.7 4.7.0 00.19 1.48 7.73 7.73.0 00.54 1.19.77.77.0 01.12.38.64.64.0 01-.32.49l-1 .7a.83.83.0 01-.44.15.69.69.0 01-.49-.23 3.8 3.8.0 01-.6-.77q-.25-.42-.51-1a6.14 6.14.0 01-4.89 2.3 4.54 4.54.0 01-3.32-1.19 4.27 4.27.0 01-1.22-3.2 4.28 4.28.0 011.46-3.4A6.06 6.06.0 017.69 6.46a12.47 12.47.0 011.76.13q.92.13 1.91.36V5.73a3.65 3.65.0 00-.79-2.66A3.81 3.81.0 007.86 2.3a7.71 7.71.0 00-1.79.22 12.78 12.78.0 00-1.79.57 4.55 4.55.0 01-.58.22h-.26q-.35.0-.35-.52V2a1.09 1.09.0 01.12-.58 1.2 1.2.0 01.47-.35A10.88 10.88.0 015.77.32 10.19 10.19.0 018.36.0a6 6 0 014.35 1.35 5.49 5.49.0 011.38 4.09zM7.34 13.38a5.36 5.36.0 001.72-.31A3.63 3.63.0 0010.63 12 2.62 2.62.0 0011.19 11a5.63 5.63.0 00.16-1.44v-.7a14.35 14.35.0 00-1.53-.28 12.37 12.37.0 00-1.56-.1 3.84 3.84.0 00-2.47.67A2.34 2.34.0 005 11a2.35 2.35.0 00.61 1.76A2.4 2.4.0 007.34 13.38zm13.35 1.8a1 1 0 01-.64-.16 1.3 1.3.0 01-.35-.65L15.81 1.51a3 3 0 01-.15-.67.36.36.0 01.41-.41H17.7a1 1 0 01.65.16 1.4 1.4.0 01.33.65l2.79 11 2.59-11A1.17 1.17.0 0124.39.6a1.1 1.1.0 01.67-.16H26.4a1.1 1.1.0 01.67.16 1.17 1.17.0 01.32.65L30 12.39 32.88 1.25A1.39 1.39.0 0133.22.6a1 1 0 01.65-.16h1.54a.36.36.0 01.41.41 1.36 1.36.0 010 .26 3.64 3.64.0 01-.12.41l-4 12.86a1.3 1.3.0 01-.35.65 1 1 0 01-.64.16H29.25a1 1 0 01-.67-.17 1.26 1.26.0 01-.32-.67L25.67 3.64l-2.56 10.7a1.26 1.26.0 01-.32.67 1 1 0 01-.67.17zm21.36.44a11.28 11.28.0 01-2.56-.29 7.44 7.44.0 01-1.92-.67 1 1 0 01-.61-.93v-.84q0-.52.38-.52a.9.9.0 01.31.06l.42.17a8.77 8.77.0 001.83.58 9.78 9.78.0 002 .2 4.48 4.48.0 002.43-.55 1.76 1.76.0 00.86-1.57 1.61 1.61.0 00-.45-1.16A4.29 4.29.0 0043 9.22l-2.41-.76A5.15 5.15.0 0138 6.78a3.94 3.94.0 01-.83-2.41 3.7 3.7.0 01.45-1.85 4.47 4.47.0 011.19-1.37 5.27 5.27.0 011.7-.86A7.4 7.4.0 0142.6.0a8.87 8.87.0 011.12.07q.57.07 1.08.19t.95.26a4.27 4.27.0 01.7.29 1.59 1.59.0 01.49.41.94.94.0 01.15.55v.79q0 .52-.38.52a1.76 1.76.0 01-.64-.2 7.74 7.74.0 00-3.2-.64 4.37 4.37.0 00-2.21.47 1.6 1.6.0 00-.79 1.48 1.58 1.58.0 00.49 1.18 4.94 4.94.0 001.83.92L44.55 7a5.08 5.08.0 012.57 1.6A3.76 3.76.0 0147.9 11a4.21 4.21.0 01-.44 1.93 4.4 4.4.0 01-1.21 1.47 5.43 5.43.0 01-1.85.93A8.25 8.25.0 0142.05 15.62z"/><path class="cls-2" d="M45.19 23.81C39.72 27.85 31.78 30 25 30A36.64 36.64.0 01.22 20.57c-.51-.46-.06-1.09.56-.74A49.78 49.78.0 0025.53 26.4 49.23 49.23.0 0044.4 22.53C45.32 22.14 46.1 23.14 45.19 23.81z"/><path class="cls-2" d="M47.47 21.21c-.7-.9-4.63-.42-6.39-.21-.53.06-.62-.4-.14-.74 3.13-2.2 8.27-1.57 8.86-.83s-.16 5.89-3.09 8.35c-.45.38-.88.18-.68-.32C46.69 25.8 48.17 22.11 47.47 21.21z"/></svg></a></div><div class=searchbox><label for=search-by><i class="fas fa-search"></i></label>
<input data-search-input id=search-by type=search placeholder=Search...>
<span data-search-clear><i class="fas fa-times"></i></span></div><script type=text/javascript src=https://tiendon.github.io/aws-worklog/js/lunr.min.js?1765436011></script><script type=text/javascript src=https://tiendon.github.io/aws-worklog/js/auto-complete.js?1765436011></script><script type=text/javascript>var baseurl="https://tiendon.github.io/aws-worklog/"</script><script type=text/javascript src=https://tiendon.github.io/aws-worklog/js/search.js?1765436011></script></div><div class=highlightable><ul class=topics><li data-nav-id=/1-worklog/ title=Worklog class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/><b>1. </b>Worklog
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/1-worklog/1.1-week1/ title="Week 1 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.1-week1/><b>1.1. </b>Week 1 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.2-week2/ title="Week 2 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.2-week2/><b>1.2. </b>Week 2 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.3-week3/ title="Week 3 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.3-week3/><b>1.3. </b>Week 3 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.4-week4/ title="Week 4 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.4-week4/><b>1.4. </b>Week 4 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.5-week5/ title="Week 5 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.5-week5/><b>1.5. </b>Week 5 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.6-week6/ title="Week 6 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.6-week6/><b>1.6. </b>Week 6 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.7-week7/ title="Week 7 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.7-week7/><b>1.7. </b>Week 7 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.8-week8/ title="Week 8 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.8-week8/><b>1.8. </b>Week 8 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.9-week9/ title="Week 9 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.9-week9/><b>1.9. </b>Week 9 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.10-week10/ title="Week 10 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.10-week10/><b>1.10. </b>Week 10 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.11-week11/ title="Week 11 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.11-week11/><b>1.11. </b>Week 11 Worklog
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/1-worklog/1.12-week12/ title="Week 12 Worklog" class=dd-item><a href=https://tiendon.github.io/aws-worklog/1-worklog/1.12-week12/><b>1.12. </b>Week 12 Worklog
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/2-proposal/ title=Proposal class=dd-item><a href=https://tiendon.github.io/aws-worklog/2-proposal/><b>2. </b>Proposal
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/ title="Translated Blogs" class="dd-item
parent"><a href=https://tiendon.github.io/aws-worklog/3-blogstranslated/><b>3. </b>Translated Blogs
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/3-blogstranslated/3.1-blog1/ title="Blog 1" class=dd-item><a href=https://tiendon.github.io/aws-worklog/3-blogstranslated/3.1-blog1/><b>3.1. </b>Blog 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.2-blog2/ title="Blog 2" class="dd-item
active"><a href=https://tiendon.github.io/aws-worklog/3-blogstranslated/3.2-blog2/><b>3.2. </b>Blog 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.3-blog3/ title="Blog 3" class=dd-item><a href=https://tiendon.github.io/aws-worklog/3-blogstranslated/3.3-blog3/><b>3.3. </b>Blog 3
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/3-blogstranslated/3.4-blog4/ title="Blog 4" class=dd-item><a href=https://tiendon.github.io/aws-worklog/3-blogstranslated/3.4-blog4/><b>3.4. </b>Blog 4
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/4-eventparticipated/ title="Events Participated" class=dd-item><a href=https://tiendon.github.io/aws-worklog/4-eventparticipated/><b>4. </b>Events Participated
<i class="fas fa-check read-icon"></i></a><ul><li data-nav-id=/4-eventparticipated/4.1-event1/ title="Event 1" class=dd-item><a href=https://tiendon.github.io/aws-worklog/4-eventparticipated/4.1-event1/><b>4.1. </b>Event 1
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.2-event2/ title="Event 2" class=dd-item><a href=https://tiendon.github.io/aws-worklog/4-eventparticipated/4.2-event2/><b>4.2. </b>Event 2
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.3-event3/ title="Event 3" class=dd-item><a href=https://tiendon.github.io/aws-worklog/4-eventparticipated/4.3-event3/><b>4.3. </b>Event 3
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.4-event4/ title="Event 4" class=dd-item><a href=https://tiendon.github.io/aws-worklog/4-eventparticipated/4.4-event4/><b>4.4. </b>Event 4
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.5-event5/ title="Event 5" class=dd-item><a href=https://tiendon.github.io/aws-worklog/4-eventparticipated/4.5-event5/><b>4.5. </b>Event 5
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/4-eventparticipated/4.6-event6/ title="Event 6" class=dd-item><a href=https://tiendon.github.io/aws-worklog/4-eventparticipated/4.6-event6/><b>4.6. </b>Event 6
<i class="fas fa-check read-icon"></i></a></li></ul></li><li data-nav-id=/5-workshop/ title=Workshop class=dd-item><a href=https://tiendon.github.io/aws-worklog/5-workshop/><b>5. </b>Workshop
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/6-self-evaluation/ title=Self-Assessment class=dd-item><a href=https://tiendon.github.io/aws-worklog/6-self-evaluation/><b>6. </b>Self-Assessment
<i class="fas fa-check read-icon"></i></a></li><li data-nav-id=/7-feedback/ title="Sharing and Feedback" class=dd-item><a href=https://tiendon.github.io/aws-worklog/7-feedback/><b>7. </b>Sharing and Feedback
<i class="fas fa-check read-icon"></i></a></li></ul><section id=shortcuts><h3>More</h3><ul><li><a class=padding href=https://www.facebook.com/groups/awsstudygroupfcj/><i class='fab fa-facebook'></i> AWS Study Group</a></li></ul></section><section id=prefooter><hr><ul><li><a class=padding><i class="fas fa-language fa-fw"></i><div class=select-style><select id=select-language onchange="location=this.value"><option id=en value=https://tiendon.github.io/aws-worklog/3-blogstranslated/3.2-blog2/ selected>English</option><option id=vi value=https://tiendon.github.io/aws-worklog/vi/3-blogstranslated/3.2-blog2/>Tiếng Việt</option></select><svg id="Capa_1" xmlns:xlink="http://www.w3.org/1999/xlink" width="255" height="255" viewBox="0 0 255 255" style="enable-background:new 0 0 255 255"><g><g id="arrow-drop-down"><polygon points="0,63.75 127.5,191.25 255,63.75"/></g></g></svg></div></a></li><li><a class=padding href=# data-clear-history-toggle><i class="fas fa-history fa-fw"></i> Clear History</a></li></ul></section><section id=footer><left><b>Workshop</b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7920860&style=0038&nbdigits=9&type=page&initCount=0" title=Migrate alt="web counter" border=0></a><br><b><a href=https://cloudjourney.awsstudygroup.com/>Cloud Journey</a></b><br><img src="https://hitwebcounter.com/counter/counter.php?page=7830807&style=0038&nbdigits=9&type=page&initCount=0" title="Total CLoud Journey" alt="web counter" border=0>
</left><left><br><br><b>Last Updated</b><br><i><span id=lastUpdated style=color:orange></span>
</i><script>const today=new Date,formattedDate="10/12/2025";document.getElementById("lastUpdated").textContent=formattedDate</script></left><left><br><br><b>Team</b><br><i><a href=https://www.facebook.com/groups/660548818043427 style=color:orange>First Cloud Journey</a><br></i></left><script async defer src=https://buttons.github.io/buttons.js></script></section></div></nav><section id=body><div id=overlay></div><div class="padding highlightable"><div><div id=top-bar><div id=breadcrumbs itemscope itemtype=http://data-vocabulary.org/Breadcrumb><span id=sidebar-toggle-span><a href=# id=sidebar-toggle data-sidebar-toggle><i class="fas fa-bars"></i>
</a></span><span id=toc-menu><i class="fas fa-list-alt"></i></span>
<span class=links><a href=https://tiendon.github.io/aws-worklog/>Internship Report</a> > <a href=https://tiendon.github.io/aws-worklog/3-blogstranslated/>Translated Blogs</a> > Blog 2</span></div><div class=progress><div class=wrapper><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#llm-observational-use-cases>LLM Observational Use Cases</a><ul><li><a href=#complexity-of-multi-model-tracing>Complexity of Multi-Model Tracing</a></li><li><a href=#predictive-operations-of-ai-workloads--cost-and-performance>Predictive Operations of AI Workloads – Cost and Performance</a></li><li><a href=#barrier-analysis>Barrier Analysis</a></li><li><a href=#data-governance-compliance-and-auditing>Data Governance, Compliance, and Auditing</a></li><li><a href=#dynatrace--llm-observation-layers>Dynatrace + LLM Observation Layers</a></li></ul></li><li><a href=#summary>Summary</a></li><li><a href=#dynatrace--aws-partner-spotlight>Dynatrace – AWS Partner Spotlight</a></li></ul></nav></div></div></div></div><div id=head-tags></div><div id=body-inner><h1>Blog 2</h1><h1 id=enhance-llm-observability-with-amazon-bedrock-and-dynatrace>Enhance LLM observability with Amazon Bedrock and Dynatrace</h1><p><strong>Author:</strong></p><ul><li>Kristof Muhi, Principal Product Manager – Dynatrace</li><li>Varun Jasti, Solutions Architect – AWS</li><li>Shashiraj Jeripotula, Principal Partner Solutions Architect – AWS</li></ul><p><em>Date: March 27, 2025</em></p><style>body{font-family:times new roman,Times,serif;font-size:13px}h1{font-size:24px}h2{font-size:18px}h3{font-size:16px}</style><h2 id=introduction>Introduction</h2><p>Organizations leveraging <a href=https://aws.amazon.com/bedrock/>Amazon Bedrock</a> for their AI generation applications need to ensure reliable, secure, and responsible AI operations at scale. As these applications become an integral part of business processes, the deployment of comprehensive Large Language Model (LLM) observability becomes essential. Monitoring model performance, detecting hallucinations, prompt injection attacks, malicious language, and PII leaks, while also tracking latency, drift, data flow, and maintaining cost control, are some of the critical use cases. By implementing robust observability practices, teams can gain deep insights into their LLM application behavior, optimize resource usage, ensure consistent response quality, and maintain compliance with governance requirements.</p><p><a href=https://www.dynatrace.com/>Dynatrace</a> is an all-in-one observation platform that automatically collects production insights, tracing, logging, metrics, and real-time application data at scale. With its powerful AI engine (<a href=https://www.dynatrace.com/platform/artificial-intelligence/>Davis AI</a>), Dynatrace alerts the team to production-level issues before they disrupt users, helps predict resource usage and costs, performance issues, and provides safeguards for data protection and compliance maintenance.</p><p>In this post, we explain how Dynatrace delivers end-to-end monitoring and visibility into AI-generated applications using Amazon Bedrock models that enable comprehensive LLM observation capabilities.</p><h2 id=llm-observational-use-cases>LLM Observational Use Cases</h2><p>Dynatrace assists with the following large-scale LLM observation and AI generation use cases.</p><h3 id=complexity-of-multi-model-tracing>Complexity of Multi-Model Tracing</h3><p>Multi-model tracing presents hidden complexities as interactions between models with different architectures, output formats, and latency profiles must be coherently correlated across the entire chain. When diverse models operate in a chain, errors can silently propagate through these heterogeneous systems, making root cause analysis particularly challenging without standardized telemetry that can efficiently connect the points between different inputs and outputs.</p><p>Dynatrace enables end-to-end tracing across different models, connecting the frontend and backend components of the application stack.</p><p>This multi-model tracing provides complete visibility and traceability of events, allowing you to understand what happened when an issue occurred or when an invalid response was sent to the client throughout the entire model chain.</p><h3 id=predictive-operations-of-ai-workloads--cost-and-performance>Predictive Operations of AI Workloads – Cost and Performance</h3><p>Predictive operations leverage advanced analytics and machine learning to predict and optimize AI workload behavior before issues impact business operations, powered by Davis AI. This proactive approach transforms traditional monitoring into forward-looking operational intelligence for AI systems.</p><p>• <strong>Cost Forecasting and Optimization:</strong> Forecast token usage, API calls, and associated costs within Amazon Bedrock to enable better budget planning and resource allocation for the future. Efficient budget planning with accurate cost forecasting helps reduce operational costs through better resource planning and allocation.</p><p>• <strong>Performance Downturn Prediction:</strong> Identify early warning signs of potential model performance issues through pattern recognition.</p><p>• <strong>Anomaly and Problem Detection:</strong> Use Dynatrace&rsquo;s Davis AI Prediction to detect anomalous patterns in model behavior that may indicate emerging problems or peak usage in the architecture. This will minimize service and application downtime by addressing potential issues before they become critical.</p><h3 id=barrier-analysis>Barrier Analysis</h3><p>Barrier analysis focuses on monitoring and enforcing safety boundaries around AI systems to ensure they operate within defined ethical, security, and performance parameters. This critical capability helps organizations maintain control over their AI applications while protecting against potential risks and abuse.</p><p>• Key components such as real-time detection of prompt injection attack attempts and security vulnerabilities protect your business and customer data. This allows organizations to monitor unauthorized PII exposures and leaks of sensitive data.</p><p>• Barriers allow you to monitor malicious and inappropriate language, harmful content, and other harmful material.
• Bias feedback with early threat detection. Allows for improved model reliability through consistent boundary enforcement and easy compliance.</p><p>• Barrier analysis protects AI applications by safeguarding brand reputation through preventing inappropriate feedback and queries.</p><h3 id=data-governance-compliance-and-auditing>Data Governance, Compliance, and Auditing</h3><p>In the context of applications built with LLM on Amazon Bedrock, governance, compliance, and auditing capabilities through observability ensure organizations maintain control, transparency, and accountability for their AI-generated applications while meeting regulatory requirements and industry standards.</p><p>Dynatrace helps track all inputs and outputs for a full audit trail. It allows you to query all data in real time and store it for future reference. It easily establishes and maintains a complete data stream from prompt to response across the entire pipeline and gathers all evidence for responsible AI practices and regulatory reporting such as <a href=https://www.nist.gov/standardsgov/compliance-faqs-federal-information-processing-standards-fips>FIPS</a>, <a href=https://www.fedramp.gov/>FedRAMP</a> and <a href=https://artificialintelligenceact.eu/>EU AI Act</a>.</p><p>Dynatrace&rsquo;s model fingerprinting capability generates unique identifiers for LLM instances based on architecture, training data, and parameters, enabling accurate instance tracking for regulatory and audit compliance. Accurate tracking of model instances through model fingerprinting ensures compliance with regulatory and audit requirements.</p><h3 id=dynatrace--llm-observation-layers>Dynatrace + LLM Observation Layers</h3><p>LLM observation requires an expansive approach spanning multiple layers, from user-oriented applications to underlying infrastructure. Each layer plays a crucial role in understanding LLM performance, identifying bottlenecks, ensuring reliable operation, and detecting potential security risks. Dynatrace provides a unified end-to-end observation platform that can help organizations gain deep insights into each of these layers, enabling them to effectively monitor, optimize, troubleshoot, and secure their LLM-powered applications.</p><p><img alt="Figure 1: Bedrock Observation Pipeline of a Travel Application Running on Kubernetes Using Dynatrace" src=https://d2908q01vomqb2.cloudfront.net/77de68daecd823babbb58edb1c8e14d7106e83bb/2025/03/27/Picture1-17.png></p><p><em>Figure 1: Bedrock Observation Pipeline of a Travel Application Running on Kubernetes Using Dynatrace</em></p><p>In our example, the application runs in a Kubernetes cluster. <a href=https://github.com/traceloop/openllmetry>OpenLLMetry</a> of <a href=https://www.traceloop.com/>Traceloop</a> enhances LLM observation capabilities for Amazon Bedrock models by collecting specific KPIs about key AI. It enriches <a href=https://opentelemetry.io/>OpenTelemetry</a> data and integrates seamlessly with Dynatrace. This provides a comprehensive view of LLM application performance in a production environment. Ultimately, it empowers businesses to optimize and scale their AI deployments efficiently.</p><p><img alt="Figure 2: High-level overview of the different layers of AI-generated measurement devices for observational capabilities" src=https://d2908q01vomqb2.cloudfront.net/77de68daecd823babbb58edb1c8e14d7106e83bb/2025/03/26/bedrock-highscale-1.png></p><p><em>Figure 2: High-level overview of the different layers of AI-generated measurement devices for observational capabilities</em></p><p>From the diagram above (Figure 2), Dynatrace provides end-to-end visibility of AI applications through the entire technology stack.</p><p>• <strong>Application Layer:</strong> Dynatrace monitors user-oriented applications interacting with LLM, tracking performance metrics, user experience, and usage patterns. Continuous data collection across the entire architecture (frontend, backend, AI generation stack) reveals real-time application behavior, while logging collects user interactions and application-specific errors for debugging. Visualization tools provide customizable dashboards to monitor key application metrics and identify trends or anomalies related to LLM integration.</p><p>• <strong>Organization Layer – Monitoring the Performance of the Orchestration Framework:</strong> These frameworks (e.g., <a href=https://www.langchain.com/>LangChain</a>, <a href=https://www.llamaindex.ai/>LlamaIndex</a>) manage the workflow prompt and integration pipeline. Dynatrace observes these workflows, providing metrics on prompt engineering efficiency, chain performance, and caching. Its anomaly detection capabilities alert the team to potential issues and bottlenecks, ensuring the smooth operation of AI-driven processes.</p><p>• <strong>Semantic Layer and Vector Databases:</strong> Analyzes the meaning and content of LLM inputs and outputs. This involves understanding relationships between concepts, tracking sentiment, and identifying potential biases, inaccuracies, or anomalies along with performance bottlenecks in Retrieval Augmented Generation (RAG) architectures using vector databases (e.g., Pinecone, Milvus, Weaviate, Qdrant, Chroma). Dynatrace&rsquo;s extensibility allows for integration.
Compatible with semantic analytics tools. By gathering data from a vector database, Dynatrace can provide a unified view of LLM output, including sentiment analysis, topic modeling, and bias detection. This data can then be visualized and analyzed using Dynatrace&rsquo;s Metrics & Performance Analytics and Visualization capabilities.</p><p>• <strong>Model Layer – Observing Model Providers and Platform Providers:</strong> Dynatrace monitors token usage, stability, latency, throughput, resource consumption, and model drift within Amazon Bedrock. Metrics & Performance Analytics allows for deep dives into model performance, tracking metrics such as latency, throughput, and resource consumption. Model fingerprinting supports detailed versioning, while anomaly detection flags significant changes in performance or output quality—helping the team understand and optimize model behavior.</p><p>• <strong>Infrastructure Layer:</strong> Dynatrace&rsquo;s full-stack observation capabilities extend to computing resources (e.g., Amazon EC2, NVIDIA GPUs) and networks. It automatically collects CPU/GPU utilization, memory usage, and other critical statistics. Real-time anomaly detection with Davis AI helps teams quickly address hardware bottlenecks that could impact LLM performance.</p><p>&ldquo;From initial model evaluation to production deployment, comprehensive monitoring is critical for generative AI systems. The integration between Dynatrace and Amazon Bedrock allows organizations to easily track key performance metrics and trace data, ensuring their AI applications remain optimized and operate reliably.&rdquo; – Denis Batalov, Tech Leader, ML & AI, AWS.</p><p><em>&ldquo;Generative AI is rapidly becoming the standard for customer experience, driving companies to deliver AI-native interactions at high speed. Simultaneously, we are witnessing an accelerated development of AI systems, leading to exponentially increasing capabilities. However, deploying these highly complex AI application stacks in production presents significant challenges. AI observational capabilities play a crucial role in ensuring reliable performance, enhancing customer satisfaction, and driving measurable ROI for the business.&rdquo;</em> – Alois Reitbauer, VP, Chief Technology Strategist, Dynatrace</p><h2 id=summary>Summary</h2><p>In this blog post, we discussed how Dynatrace can deliver enhanced visibility into generative AI applications leveraging Amazon Bedrock.</p><p>Leveraging Dynatrace&rsquo;s capabilities, you can:</p><p>• <strong>Maintain operational efficiency:</strong> Monitor latency, resource usage, and costs to optimize performance and control expenses.</p><p>• <strong>Accelerate the path to production:</strong> Deploy reliable and secure AI applications faster with confidence.</p><p>• <strong>Make data-driven decisions:</strong> Leverage comprehensive data to inform model selection, fine-tuning, and risk mitigation strategies.</p><p>• <strong>Improve application reliability:</strong> Proactively identify and address performance bottlenecks and other issues before they impact users.</p><p>• <strong>Enhance security:</strong> Detect and mitigate security risks in real time, protecting your data and reputation.</p><p>• <strong>Enhancing Governance and Compliance:</strong> Maintain a clear data flow and ensure responsible AI practices that meet regulatory requirements and ethical standards.</p><p>For guidance on setting up Dynatrace&rsquo;s end-to-end instrumentation solution with Amazon Bedrock, please refer to the following Dynatrace blog post.</p><h2 id=dynatrace--aws-partner-spotlight>Dynatrace – AWS Partner Spotlight</h2><p>Dynatrace is an AWS Advanced Technology Partner and AWS Competency Partner providing intelligent software to simplify cloud complexity and accelerate digital transformation. With enhanced observational capabilities, AI, and full automation, our all-in-one platform provides answers—not just data—about application performance, underlying infrastructure, and the experience of all users.</p><p><a href="https://partnercentral.awspartner.com/PartnerConnect?id=001E000000texmiIAA&source=Blog&campaign=">Contact Dynatrace</a> | <a href=https://partners.amazonaws.com/partners/001E000000texmiIAA/Dynatrace>Partner Overview</a> | <a href="https://aws.amazon.com/marketplace/seller-profile?id=1422b3b0-b081-4af9-9d2b-34e6eb924f05">AWS Marketplace</a></p><p><strong>TAGS:</strong> <a href=https://aws.amazon.com/blogs/apn/tag/amazon-bedrock/>Amazon Bedrock</a>, <a href=https://aws.amazon.com/blogs/apn/tag/artificial-intelligence/>Artificial Intelligence</a>, <a href=https://aws.amazon.com/blogs/apn/tag/machine-learning/>Machine Learning</a>, <a href=https://aws.amazon.com/blogs/apn/tag/observability/>Observability</a></p><footer class=footline></footer></div></div><div id=navigation><a class="nav nav-prev" href=https://tiendon.github.io/aws-worklog/3-blogstranslated/3.1-blog1/ title="Blog 1"><i class="fa fa-chevron-left"></i></a>
<a class="nav nav-next" href=https://tiendon.github.io/aws-worklog/3-blogstranslated/3.3-blog3/ title="Blog 3" style=margin-right:0><i class="fa fa-chevron-right"></i></a></div></section><div style=left:-1000px;overflow:scroll;position:absolute;top:-1000px;border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px><div style=border:none;box-sizing:content-box;height:200px;margin:0;padding:0;width:200px></div></div><script src=https://tiendon.github.io/aws-worklog/js/clipboard.min.js?1765436011></script><script src=https://tiendon.github.io/aws-worklog/js/perfect-scrollbar.min.js?1765436011></script><script src=https://tiendon.github.io/aws-worklog/js/perfect-scrollbar.jquery.min.js?1765436011></script><script src=https://tiendon.github.io/aws-worklog/js/jquery.sticky.js?1765436011></script><script src=https://tiendon.github.io/aws-worklog/js/featherlight.min.js?1765436011></script><script src=https://tiendon.github.io/aws-worklog/js/highlight.pack.js?1765436011></script><script>hljs.initHighlightingOnLoad()</script><script src=https://tiendon.github.io/aws-worklog/js/modernizr.custom-3.6.0.js?1765436011></script><script src=https://tiendon.github.io/aws-worklog/js/learn.js?1765436011></script><script src=https://tiendon.github.io/aws-worklog/js/hugo-learn.js?1765436011></script><link href=https://tiendon.github.io/aws-worklog/mermaid/mermaid.css?1765436011 rel=stylesheet><script src=https://tiendon.github.io/aws-worklog/mermaid/mermaid.js?1765436011></script><script>mermaid.initialize({startOnLoad:!0})</script><script>(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,(e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date),(i=t.createElement(n),a=t.getElementsByTagName(n)[0]),i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-158079754-2","auto"),ga("send","pageview")</script></body></html>